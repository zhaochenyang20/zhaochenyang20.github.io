<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Segement Me If U Can | 你一生的故事</title><meta name="keywords" content="2022春季,科研"><meta name="author" content="Eren Zhao"><meta name="copyright" content="Eren Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="勿因赢小而不麻，勿因恩小而不感，勿因乐小而不偷...">
<meta property="og:type" content="article">
<meta property="og:title" content="Segement Me If U Can">
<meta property="og:url" content="http://example.com/2022/06/06/Lecture/2022%20Spring/deep_learning/index.html">
<meta property="og:site_name" content="你一生的故事">
<meta property="og:description" content="勿因赢小而不麻，勿因恩小而不感，勿因乐小而不偷...">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.imgdb.cn/item/61f0fc722ab3f51d91730c62.jpg">
<meta property="article:published_time" content="2022-06-06T08:50:44.589Z">
<meta property="article:modified_time" content="2022-08-26T16:18:34.678Z">
<meta property="article:author" content="Eren Zhao">
<meta property="article:tag" content="2022春季">
<meta property="article:tag" content="科研">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/61f0fc722ab3f51d91730c62.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://example.com/2022/06/06/Lecture/2022%20Spring/deep_learning/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"麻了，找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Eren Zhao","link":"链接: ","source":"来源: 你一生的故事","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Segement Me If U Can',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-08-27 00:18:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><meta name="generator" content="Hexo 6.0.0"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}</style></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend.jpg'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">190</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">51</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">30</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.imgdb.cn/item/61f0fc722ab3f51d91730c62.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">你一生的故事</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Segement Me If U Can</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-06-06T08:50:44.589Z" title="发表于 2022-06-06 16:50:44">2022-06-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-26T16:18:34.678Z" title="更新于 2022-08-27 00:18:34">2022-08-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B/">课程</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AF%BE%E7%A8%8B/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>18分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Segement Me If U Can"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="segement-me-if-u-can"><strong>Segement Me If U Can</strong></h1>
<p>THUCST Intro to AI PA 2</p>
<p><a target="_blank" rel="noopener" href="https://zhaochenyang20.github.io/">Eren zhao</a> Class 06</p>
<h1 id="任务简介">任务简介</h1>
<ul>
<li>进行一次情感二分类，仅考虑正负情感。</li>
</ul>
<h2 id="实验数据">实验数据</h2>
<ul>
<li>实验数据包括包含训练、验证、测试集合以及预处理好的词向量</li>
<li>句子的分类包含正向和负向两种</li>
</ul>
<h2 id="实验要求">实验要求</h2>
<ul>
<li>本次实验要求实现 CNN 与 RNN 两个模型，并应用在情感分类任务上。RNN 可以是 LSTM，GRU 等类型。</li>
<li>对比两模型的实验效果，并分析原因。 也可以实现其他模型作为对比模型（baseline），例如全连接神经网络（MLP），可适当加分。</li>
</ul>
<h2 id="评价指标">评价指标</h2>
<ol type="1">
<li>准确率（Accuracy）</li>
<li><a target="_blank" rel="noopener" href="https://deepai.org/machine-learning-glossary-and-terms/f-score">F-score</a>，类似 MIOU</li>
</ol>
<h2 id="报告内容">报告内容</h2>
<ol type="1">
<li>模型的结构图，以及流程分析。</li>
<li>实验结果，准确率，F-score 的实验效果。</li>
<li>试简要地比较实验中使用的不同参数效果，并分析原因。</li>
<li>比较 baseline 模型与 CNN，RNN 模型的效果差异。（如果有实现）</li>
<li>问题思考，心得体会</li>
</ol>
<h2 id="question-list">Question List</h2>
<ol type="1">
<li>实验训练什么时候停止是最合适的？简要陈述你的实现方式，并试分析固定迭代次数与通过验证集调整等方法的优缺点。</li>
<li>实验参数的初始化是怎么做的？不同的方法适合哪些地方？（现有的初始化方法为零均值初始化，高斯分布初始化，正交初始化等）</li>
<li>过拟合是深度学习常见的问题，有什么方法可以防止训练过程陷入过拟合</li>
<li>试分析CNN，RNN，全连接神经网络（MLP）三者的优缺点</li>
</ol>
<h1 id="模型结构">模型结构</h1>
<h2 id="bi-lstm">Bi-LSTM</h2>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/LSTM.jpg" alt="LSTM" /><figcaption aria-hidden="true">LSTM</figcaption>
</figure>
<ul>
<li>双向 LSTM 分类网络的模型结果如上图。前向传播的流程为：将一批长度统一且标记化的句子输入网络，依次经过：</li>
</ul>
<ol type="1">
<li>嵌入层：将每个表示单词的自然数映射为指定长度的向量，即用向量表示单词。</li>
<li>双向双层 LSTM 层：接收某个 batch 的词向量组成的序列，每个 LSTM 单元在两个方向上分别产生自己的隐藏状态。最终只用了最后一层（第二层）两个方向上传播的各自的最后一个单元的隐藏状态作为下一层的输入。</li>
<li>线性分类层：由两层网络构成，接收上述 LSTM 层产生的两个隐藏状态直接拼接起来的向量（维数变为隐藏状态维数的 2 倍）作为输入，经过两层线性层输出维数等于分类类别数的向量，表示对类别的预测结果。</li>
<li>在 Bi-LSTM 的基础上，将 LSTM 单元替换为 GRU 单元即可得到基于 LSTM 变种 <a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be">GRU</a> 实现的 RNN，此处不再赘述。</li>
</ol>
<figure>
<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/GRU.svg" alt="GRU" /><figcaption aria-hidden="true">GRU</figcaption>
</figure>
<h2 id="text-cnn">Text-CNN</h2>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/CNN.jpg" alt="CNN" style="zoom:50%;" /></p>
<ul>
<li>依据<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1408.5882">参考文献</a>中的模型搭建 Text-CNN 模型。前向传播流程如下：</li>
</ul>
<ol type="1">
<li>嵌入层：将每个表示单词的自然数映射为指定长度的向量，即用向量表示单词。</li>
<li>一维多通道多卷积核卷积层：将嵌入层得到的数据视为一批多通道的一维张量；一维张量的长度为对齐后的句子长度，通道数为词向量的数。用指定数量与大小的卷积核与输入数据做多通道多卷积核卷积，得到多通道的一维输出特征。用宽度为3、5、7的卷积核分别做三次卷积。</li>
<li>池化层：对卷积结果进行 activate, Dropout, max pooling。</li>
<li>线性层：将池化后的卷积结果拼接在一起，得到长度为所有卷积输出通道数之和的张量，再经过一层线性层得到表示类别标签预测的向量。</li>
</ol>
<h2 id="mlp">MLP</h2>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/MLP.jpg" alt="MLP" style="zoom:50%;" /></p>
<p>使用 MLP 作为 baseline 。模型示意图如上，前向传播大致流程如下： 1. 嵌入层：将每个表示单词的自然数映射为指定长度的向量，即用向量表示单词。 2. 线性层1：接收一批将句子中的词向量直接拼接起来得到的张量为输入，输出指定大小的张量，然后进行 Batch Normalization, Activation, Dropout。 3. 线性层2：输出表示类别标签预测的向量。</p>
<h1 id="配置信息">配置信息</h1>
<h2 id="环境库">环境库</h2>
<p>参考 requirements.txt</p>
<h2 id="可视化">可视化</h2>
<p>采用 <a target="_blank" rel="noopener" href="https://wandb.ai/site">wandb</a> 辅助可视化</p>
<h2 id="算力">算力</h2>
<p>由于我自己的电脑是 Macbook M1 Core，虽然 M1 芯片优化了 CPU 计算的性能，然而没有显卡是硬伤。于是我在自己的服务器上进行训练，服务器有 1 张 3080。</p>
<h1 id="实验结果">实验结果</h1>
<h2 id="模型对比">模型对比</h2>
<p>对比正确率以及训练过程中损失值和验证集上的正确率随训练时间推移的关系（如下图，由 wandb 生成；损失值为每优化一步时模型返回的结果，验证集的准确率为每一 epoch 衡量一次），可以对四种模型进行比较。</p>
<p>此处展示 f1_score 和 accuracy 与 CrossEntropyLoss 的 loss。</p>
<h3 id="accuracy">accuracy</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/acc1.png" style="zoom:13%;" /></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/acc2.png" style="zoom:13%;" /></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/acc3.png" style="zoom:13%;" /></p>
<h3 id="f1_score">f1_score</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/f11.png" style="zoom:13%;" /></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/f12.png" style="zoom:13%;" /></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/f13.png" style="zoom:13%;" /></p>
<h3 id="loss">loss</h3>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/loss1.png" style="zoom:13%;" /></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/loss2.png" style="zoom:13%;" /></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/loss3.png" style="zoom:13%;" /></p>
<p>其中对于 f1_score 与 accuracy，四个模型在我所选取的参数下的最佳结果为：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Text-CNN</th>
<th>RNN-GRU</th>
<th>RNN-LSTM</th>
<th>MLP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>accuracy</td>
<td>0.8618</td>
<td>0.8482</td>
<td>0.8455</td>
<td>0.813</td>
</tr>
<tr class="even">
<td>f1_score</td>
<td>0.864</td>
<td>0.8526</td>
<td>0.8512</td>
<td>0.8089</td>
</tr>
</tbody>
</table>
<p>结合评价指标与上图，可以观察出：</p>
<ul>
<li>RNN 的损失值下降最快，收敛最快，正确率在 Text-CNN 与 MLP 之间，然而过拟合现象出现最快</li>
<li>CNN 的损失值下降居中，正确率最高，过拟合现象最不明显（经过 100 个 epoch 的实验证明，Text-CNN 的正确率从 25 个 epoch 左右开始收敛到 0.847）</li>
<li>MLP 损失函数下降速度、收敛速度最慢，正确率最低，过拟合的程度居于 Text-CNN 与 RNN 之间。</li>
</ul>
<h2 id="参数对比">参数对比</h2>
<p>大量的参数让我感到调参工作的艰巨，最让我感到震撼的是我对 max_length 参数的调配。</p>
<h3 id="max_length">max_length</h3>
<p>Max_length 参数用于控制句子的最长词向量个数，我起初将其设置为 50，也即设置为与词向量维度相同，这样以来，input matrix 的 size 为 50*50，这样做的效果如下表格所示：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Text-CNN</th>
<th>RNN-GRU</th>
<th>RNN-LSTM</th>
<th>MLP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>accuracy</td>
<td>0.8618</td>
<td>0.8482</td>
<td>0.8455</td>
<td>0.813</td>
</tr>
<tr class="even">
<td>f1_score</td>
<td>0.864</td>
<td>0.8526</td>
<td>0.8512</td>
<td>0.8089</td>
</tr>
</tbody>
</table>
<p>然而，我与同学交流后得知，经过人工核实数据集后发现，只有一条语句长度为 600+，其余语句长度均低于 120。转念一想，我认为自己丢失了大量的语句信息，于是考虑将 max_length 调整为 120。期望能够为模型带来巨大突破，然而结果如下：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>Text-CNN</th>
<th>RNN-GRU</th>
<th>RNN-LSTM</th>
<th>MLP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>accuracy</td>
<td>0.8509</td>
<td>0.8564</td>
<td>0.8509</td>
<td>0.8347</td>
</tr>
<tr class="even">
<td>f1_score</td>
<td>0.8501</td>
<td>0.8638</td>
<td>0.8415</td>
<td>0.8329</td>
</tr>
</tbody>
</table>
<p>实际上并未出现我所设想的巨大突破（譬如突破到 90 点以上），甚至在 Text-CNN 上反而结果下降。起初，我认为一个句子仅仅截断前 50 个单位是很难表达句子全意的，很有可能会误判“欲扬先抑”这样的语义。然而再一设想，考虑到此处的词向量单位是汉语中的词语而非汉字，实际上在较短篇幅的评论中，即便是欲扬先抑，也很难会有超过 50 个词语的铺垫，反而由于输入矩阵的大小倍增，导整体的计算效率有一定降低。再三思考，我并未取用 max_length = 120，而是保持了 50。</p>
<p>真正决定了训练效果无法突破 90 大关的是模型本身与预训练模型，而这些精细的调整并未能触碰到 90 大关。</p>
<h3 id="初始学习率">初始学习率</h3>
<p>以 CNN 为例，调整 Adam 优化器的初始学习率，检测测试集上的正确率： <img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/test_learning_rate.png" style="zoom:13%;" /></p>
<p>可见，Adam 优化器默认提供的 0.001 的学习率是一个比较好的值。学习率过大或者过小都会影响训练效果；前者使得神经网络每次优化跨度太大，反而难以找到极小值；后者会使神经网络收敛速度太慢。</p>
<h1 id="问题思考">问题思考</h1>
<h2 id="实验停止">实验停止</h2>
<p>实验训练什么时候停止是最合适的？简要陈述你的实现方式，并试分析固定迭代次数与通过验证集调整等方法的优缺点。</p>
<p>在尚未确定以 10 为最终 epoch 前，起初我设置了一个最大值闸值 max_epoch = 10。在无限次训练的情况下，倘若连续 max_epoch 次训练都没能超越之前记录的最大 test_acc 值即停止训练，然而经过数次尝试且调整 max_epoch 后，我发现训练的最大值均在前 10 个 epoch 就会取得，其后均是波动而收敛，例如经过无限个 epoch 的实验证明，Text-CNN 的正确率从 25 个 epoch 左右开始收敛到 0.847，而 MLP 从 33 个 epoch 之后开始收敛到 0.772）</p>
<p>这样的结果无疑让我感到诧异，我最终选择了仅仅保留十个 epoch 的做法，似乎这样此次实验不需追求自动化停止训练。然而在实际的科研应用中，自动化停止训练是非常必要的，否则会浪费大量的计算资源。</p>
<p>我认为，可以先简单的考虑人工观察训练情况。利用 wandb 的可视化功能进行观察。如果训练集上的 loss 在下降而验证集上的正确率在上升，则需要继续训练。直到训练集上的 loss 在下降（或者几乎不变）而验证集上的正确率也在下降，说明出现了过拟合，那么此时即可停止训练，因为继续下去得到的模型是性能表现并不好的过拟合的模型。</p>
<h2 id="参数初始化">参数初始化</h2>
<p>本次实验中，RNN 与 CNN 的参数均为 kaiming 初始化，而 MLP 采用高斯初始化。</p>
<ol type="1">
<li>在回答问题前，我先叙述两次我在本次实验当中失败的参数初始化：</li>
</ol>
<p>首先是我在 MLP 的模型设计中，采用了初始化参数全 0 与全 1 的设定，导致模型完全没法训练。后来才发现这是深度学习的经典错误。</p>
<p>此处以三层神经网络为例，分析为何不可全 0 初始化。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/lecture/2022_spring/IAI/three_layer.jpg" style="zoom:13%;" /></p>
<p>其中，<span class="math inline">\(z_4、z_5、z_6\)</span> 可表达为:</p>
<p><span class="math display">\[
z_4 = w_{14} * x_1 + w_{24} * x_2 + w_{34} * x_3 + b_4 \ z_5 = \\ w_{15} * x_1 + w_{25} * x_2 + w_{35} * x_3 + b_5 \ z_6 = w_{16} * x_1 + w_{26} * x_2 + w_{36} * x_3 + b_6 \
\]</span> 由于权重和偏置的初始值都为0，且同一层网络的激活函数相同，则有： <span class="math display">\[
z_4 = z_5 = z_6 \ a_4 = a_5 = a_6
\]</span> 对于神经网络的最终输出 <span class="math inline">\(a_7\)</span>，我们可以得到： <span class="math display">\[
\begin{align} z_7 &amp;= w_{47} * a_4 + w_{57} * a_5 + w_{67} * a_6 \ a_7 = f(z_7) \end{align}
\]</span> 其中，<span class="math inline">\(f(\cdot)\)</span>为第三层网络的激活函数。</p>
<p>假设真实值为 <span class="math inline">\(y\)</span>, 损失函数为 <span class="math inline">\(Loss(a_7, y)\)</span>，根据反向传播算法和链式法则，我们可以得到：</p>
<p><span class="math display">\[
\begin{align} \frac{\partial Loss(a_7,y)}{\partial w_{47}} &amp;= \frac{\partial Loss(a_7, y)}{\partial a_7} * \frac{\partial a_7}{\partial z_7} * \frac{\partial z_7}{\partial w_{47}} \ =\frac{\partial Loss(a_7, y)}{\partial a_7} * \frac{\partial a_7}{\partial z_7} * a_4 \end{align}
\]</span> 同样地： <span class="math display">\[
\begin{align} \frac{\partial Loss(a_7,y)}{\partial w_{57}} = \frac{\partial Loss(a_7, y)}{\partial a_7} * \frac{\partial a_7}{\partial z_7} * \frac{\partial z_7}{\partial w_{57}} \ =\\ \frac{\partial Loss(a_7, y)}{\partial a_7} * \frac{\partial a_7}{\partial z_7} * a_5 \ \frac{\partial Loss(a_7,y)}{\partial w_{67}} = \frac{\partial Loss(a_7, y)}{\partial a_7} * \frac{\partial a_7}{\partial z_7} * a_6 \end{align}
\]</span> 由于 <span class="math inline">\(a_4 = a_5 = a_6\)</span>，则有： <span class="math display">\[
\frac{\partial Loss(a_7,y)}{\partial w_{47}} = \frac{\partial Loss(a_7,y)}{\partial w_{57}} = \frac{\partial Loss(a_7,y)}{\partial w_{67}} = \Delta w 
\]</span> 权重更新表达式为： <span class="math display">\[
w_{47}^{&#39;} = w_{47} + \Delta w \ w_{57}^{&#39;} = w_{57} + \Delta w \ w_{67}^{&#39;} = w_{67} + \Delta w 
\]</span> 由于 <span class="math inline">\(w_{47}、w_{57}、w_{67}\)</span> 的初始值均为0，那么： <span class="math display">\[
w_{47}^{&#39;} = w_{57}^{&#39;} = w_{67}^{&#39;}\\w_{14}^{&#39;} = w_{24}^{&#39;} = w_{34}^{&#39;} = w_{14}^{&#39;} = w_{25}^{&#39;} = w_{26}^{&#39;} = w_{34}^{&#39;} = w_{35}^{&#39;} = w_{36}^{&#39;}
\]</span> 由此可见，更新后的参数在每一层内都是相同的。同时，无论经过多少次网络训练，相同网络层内的参数值都是相同的，这会导致网络在学习时没有重点，对所有的特征处理相同，这很可能导致模型无法收敛训练失败。这种现象被称为对称失效。</p>
<p>同样地，当权重被初始化为相同的非零值时，也会出现上述情况，此时神经网络模型和一个线性模型的效果相似，失去了神经网络提取特征的意义。</p>
<ol start="2" type="1">
<li>高斯分布初始化</li>
</ol>
<p>一种非常常见的方式是采用高斯分布，其分布的大小对于优化过程的结果和网络泛化能力都有很大影响。</p>
<p>使用一个均值为 <span class="math inline">\(\mu\)</span>，方差为 <span class="math inline">\(\sigma^2\)</span> 的高斯分布 <span class="math inline">\(N(\mu, \sigma^2)\)</span> 对每个参数进行随机初始化，通常情况下，<span class="math inline">\(\mu = 0\)</span>，并对生成的数乘上一个小数，把权重初始化为很小的随机数。比如：<span class="math inline">\(w = 0.01 * np.random.rand(D,H)\)</span>，这里选择乘以0.01初始化为一个很小的数是因为，如果最初随机到的 <span class="math inline">\(w\)</span> 值很大，当我们选择 sigmoid 或 tanh 激活函数时，函数值 <span class="math inline">\(sigmoid(\cdot)\)</span> 或 <span class="math inline">\(tanh(\cdot)\)</span> 会停留在一个很平坦的地方，激活值接近饱和，导致梯度下降时，梯度很小，学习变得缓慢。但也不是说权重值越小越好，如果权重值过小，会导致在反向传播时计算得到很小的梯度值，在不断的反向传播过程中，引起梯度消失。</p>
<ol start="3" type="1">
<li>kaiming 初始化</li>
</ol>
<p>有名的何恺明初始化。pytorch 对 conv2d 的默认初始化采用了 kaiming 初始化，这种初始化能很好的适配 ReLU 激活函数。假定使用ReLU激活函数时，网络每一层都中有一半的神经元被激活，另一半为0，因此其分布的方差也近似为恒等函数的一半。这样在考虑前向传播和反向传播时则有： <span class="math display">\[
{\forall}i, \quad \frac12 n_i * Var(W_i) = 1\ {\forall}i, \quad \frac12 n_{i+1} * Var(W_i) = 1
\]</span> <span class="math inline">\(W_i\)</span> 的理想方差为： <span class="math display">\[
 {\forall}i, \quad Var(W_i) = \frac{2}{n_i} 
\]</span></p>
<p>当采用高斯分布时，则权重可按照 <span class="math inline">\(N(0, \frac{2}{n_i})\)</span> 的高斯分布来进行初始化。若采用在区间 <span class="math inline">\([-r, r]\)</span> 的均匀分布进行初始化，则初始化分布有：</p>
<p><span class="math display">\[
W \sim U[- \frac{\sqrt 6}{\sqrt{n_i}}, \frac{\sqrt 6}{\sqrt{n_i}}]
\]</span> 具体论文参见：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1502.01852.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></p>
<ol start="4" type="1">
<li>Xavier初始化</li>
</ol>
<p>Xavier初始化，即 0 均值初始化遵循了Bradley（2009）的理论环境，假设网络中的每一层的激活函数都是关于0对称的线性激活函数，权重间的初始化和输入特征相互独立，且均值都为0。</p>
<p>假设在一个神经网络中，对于一层线性网络，其表示为： <span class="math display">\[
y = f(z_1W_1 + z_2W_2 + z_3W_3 + ... + z_iW_i + b) 
\]</span></p>
<p><span class="math inline">\(z_i\)</span> 代表该层网络的第 <span class="math inline">\(i\)</span> 个神经元，<span class="math inline">\(y\)</span> 为该层网络的输出，<span class="math inline">\(W_i\)</span> 为本层网络的权重，<span class="math inline">\(b\)</span> 为偏置，<span class="math inline">\(f(\cdot)\)</span> 为激活函数。这里我们假设激活函数为恒等函数，即 <span class="math inline">\(f(x) = x\)</span>，导数为1。</p>
<p>对于其中的每个 <span class="math inline">\(z_iW_i\)</span>，其方差为： <span class="math display">\[
Var(z_iW_i) = E(z_i)^2Var(W_i) + E(W_i)^2Var(z_i)+Var(z_i)Var(W_i) 
\]</span> 由于 <span class="math inline">\(W_i\)</span> 和 <span class="math inline">\(z_i\)</span> 的均值都为0，因此可以得到： <span class="math display">\[
Var(z_iW_i) = Var(z_i)Var(W_i)
\]</span> 又因为 <span class="math inline">\(z\)</span> 和 <span class="math inline">\(W\)</span> 相互独立，则有： <span class="math display">\[
Var(y) = n_i * Var(z_i)Var(W_i)
\]</span> 其中，<span class="math inline">\(n_i\)</span> 代表第 <span class="math inline">\(i\)</span> 层的神经元数量。</p>
<p>通过上面的公式我们可以发现，输入 <span class="math inline">\(z_i\)</span> 的方差和输出 <span class="math inline">\(y\)</span> 方差相差 <span class="math inline">\(n * Var(W_i)\)</span> 倍，也就是说输入信号在经过神经元后会被放大或缩小 <span class="math inline">\(n * Var(W_i)\)</span> 倍。为保证经过多层网络后，信号不被过分的放大或缩小，我们需要尽可能保证前向传播和反向传播时每层方差保持一致，则有： <span class="math display">\[
{\forall}i, \quad n_i * Var(W_i) = 1\ {\forall}i, \quad n_{i+1} * Var(W_i) = 1 
\]</span> 权衡上述两个限制，提出一个折中的办法： <span class="math display">\[
{\forall}i, \quad Var(W_i) = \frac{2}{n_i + n_{i+1}}
\]</span> 根据计算出的理想方差，可选择通过高斯分布或均匀分布来随机初始化参数。若采用高斯分布，则权重可按照 <span class="math inline">\(N(0, \frac{2}{n_i + n_{i+1}})\)</span> 的高斯分布来进行初始化。若采用在区间 <span class="math inline">\([-r, r]\)</span> 的均匀分布进行初始化，则初始化分布有： <span class="math display">\[
W \sim U[- \frac{\sqrt 6}{\sqrt{n_i + n_{i+1}}}, \frac{\sqrt 6}{\sqrt{n_i + n_{i+1}}}] 
\]</span></p>
<p>Xavier 初始化因为基本保证了输入和输出的方差一致，使得样本空间和类别空间的分布差异相似，因此使得模型的训练速度和分类性能获得提升。但xavier初始化的推导基于激活函数是线性的假设，使其并不适合于ReLU、sigmoid等非线性激活函数。</p>
<p>具体论文参见：<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Understanding the difficulty of training deep feedforward neural networks</a></p>
<ol start="5" type="1">
<li>正交初始化</li>
</ol>
<p>将权重初始化为正交矩阵。对于训练非常深的网络很有用。可用于帮助 RNN 中的梯度消失和爆炸。首先利用 torch 生成一个随机数矩阵，X（例如从正态分布），而后执行 QR 分解 X = QR，得到一个正交矩阵 Q 和一个上三角矩阵 R，最后用所得到的 Q 矩阵初始化。</p>
<h2 id="过拟合问题">过拟合问题</h2>
<p>产生过拟合的原因可能有：</p>
<ol type="1">
<li><p>训练集上数据量不够大。</p></li>
<li><p>参数量过大、模型过于复杂容易导致模型学习到非共性的特征，从而导致泛化能力下降，出现过拟合。</p></li>
<li><p>训练程度不够，提早结束。</p></li>
</ol>
<p>对此，有如下方案：</p>
<ol type="1">
<li>适当简化模型，减小参数量，此即 dropout 方法。</li>
<li>增加训练数据的规模来解决。也可以适用 data augmentation 将已有数据做不影响真实类别标签的微小变换产生类似的新数据，变相增大数据量。</li>
<li>通过观察 loss 和验证集上正确率的变化趋势提前停止训练，并选择 loss 几乎最小，而验证集上正确率几乎最大的模型作为最终模型的方法。</li>
<li>正则化：通过在 loss 中增加惩罚项，限制了非线性函数中高次项系数的大小，相当于避免了其过于复杂，从而提高泛化能力。</li>
</ol>
<h2 id="模型对比-1">模型对比</h2>
<p>前文所述：</p>
<blockquote>
<ul>
<li>RNN 的损失值下降最快，收敛最快，正确率在 Text-CNN 与 MLP 之间，然而过拟合现象出现最快</li>
<li>CNN 的损失值下降居中，正确率最高，过拟合现象最不明显（经过 100 个 epoch 的实验证明，Text-CNN 的正确率从 25 个 epoch 左右开始收敛到 0.847）</li>
<li>MLP 损失函数下降速度、收敛速度最慢，正确率最低，过拟合的程度居于 Text-CNN 与 RNN 之间。</li>
</ul>
</blockquote>
<p>综上可以总结如下：</p>
<table>
<colgroup>
<col style="width: 2%" />
<col style="width: 36%" />
<col style="width: 25%" />
<col style="width: 36%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>RNN</th>
<th>CNN</th>
<th>MLP</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>优点</td>
<td>输入长度可变且有序（输入携带了时序信息）</td>
<td>可以并行计算，计算速度快；可以提取局部特征</td>
<td>可并行计算，计算速度快，模型简单易上手</td>
</tr>
<tr class="even">
<td>缺点</td>
<td>有序性导致其不能并行计算，计算速度慢；梯度不稳定，容易梯度消失</td>
<td>输入长度固定，可能丢失语义信息</td>
<td>输入长度固定，可能丢失语义信息；模型上限显著，表达性不高：参数量小时表达能力不够，而为了提取更多特征增大隐藏层维数时又容易使参数量过大，导致参数利用率偏低</td>
</tr>
</tbody>
</table>
<h1 id="关于-bert">关于 Bert</h1>
<p>久仰 transformer 与 bert 的大名，我相信 BERT 已经在海量数据中训练，较好地学习到了词汇的语义，最终在此任务上只需要最后加一个简单的线性层做分类就能达到很好的效果。</p>
<p>但是这实际上抛弃 wiki_word2vec_50.bin 词向量嵌入文件，远超出了此次实验范围。我相信此次实验如若使用 bert 来完成，可以突破 90% 这一大关。</p>
<h1 id="心得体会">心得体会</h1>
<p>纸上得来终觉浅，绝知此事要躬行。在马老师的课堂上与平日实验室的工作当中，我对模型参数的初始化并不太在意，甚至我并不了解模型何时应该初始化参数，何时不必初始化。</p>
<p>在这次实验中，我首先完成了 CNN 与 RNN，这两个模型都没手动初始化。我认为这些模型默认就是不必初始化的，实际上是我没认识到 pytorch 已经采用了高效的 kaiming 初始化。</p>
<p>直到我完成 MLP 的模型才发现我起初完全错误的全 0 初始化与全 1 初始化导致了对称失效，失去了模型的表达力。我这才发现需要利用高斯初始化。转而查阅资料，我才认识到了 CNN 与 RNN 已经默认了初始化。</p>
<p>若非本次动手实验，这样精妙的细节可能会一直被我抛之脑后，直到在研究中因为这样基础的漏洞而发生严重的问题。所幸通过本次实验，我及时补上了这一漏洞，而且对参数初始化有了自己的认知。</p>
<p>从马老师的课堂所讲解的理论出发，本次实验完整的实现了一次神经网络的训练过程。从数据加载，模型构建，设置训练方式与损失函数，梯度反向传播等等角度，给予了我对于课堂知识更丰富的理解，也让我意识到了深入学习 numpy pytorch 等工具的重要性。</p>
<p>除此之外，借助 wandb，我高效地实现了精美全面的可视化，加深了对深度学习中关键问题的理解，掌握了完成深度学习任务的基本流程。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2022%E6%98%A5%E5%AD%A3/">2022春季</a><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-62b11572b25ab3ab" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/06/10/%E9%9A%8F%E7%AC%94/%E5%8D%9A%E6%96%87/singapore/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Simon Berger (5EB1k3iCsJM).jpg" onerror="onerror=null;src='https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Oscar Rodrigo Hernandez Panczenko (P0GQ95huhBo).jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">工作 3 年，我又从新加坡反向润回国了</div></div></a></div><div class="next-post pull-right"><a href="/2022/06/06/Lecture/2022%20Spring/statistic/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by John O'Nolan (6f_ANCcbj3o).jpg" onerror="onerror=null;src='https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Oscar Rodrigo Hernandez Panczenko (P0GQ95huhBo).jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Statistics</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/05/24/Lecture/2022%20Spring/Reinforcement/" title="重力四子棋"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.imgdb.cn/item/61eccb4d2ab3f51d91d616f7.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-24</div><div class="title">重力四子棋</div></div></a></div><div><a href="/2022/03/15/Lecture/2022%20Spring/Introduction_to_AI/" title="Introduction to Artificial Intelligence"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Johannes Plenio (E-Zuyev2XWo).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-15</div><div class="title">Introduction to Artificial Intelligence</div></div></a></div><div><a href="/2022/03/23/Lecture/2022%20Spring/Input%20method/" title="How Do We Train An Input Method"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Jan Kronies (pRqF0180Wj4).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-23</div><div class="title">How Do We Train An Input Method</div></div></a></div><div><a href="/2022/05/17/Lecture/2022%20Spring/binaryDivdence/" title="深度学习基础"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.imgdb.cn/item/61f106842ab3f51d917b1e63.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-17</div><div class="title">深度学习基础</div></div></a></div><div><a href="/2022/06/04/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/Gau%20GAN/" title="Understanding Gau GAN"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Johannes Plenio (E-Zuyev2XWo).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-04</div><div class="title">Understanding Gau GAN</div></div></a></div><div><a href="/2022/03/08/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/Fully%20Convolutional%20Networks%20for%20Semantic%20Segmentation/" title="Fully Convolutional Networks for Semantic Segmentation 阅读笔记"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.imgdb.cn/item/61eccb5a2ab3f51d91d621f8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-08</div><div class="title">Fully Convolutional Networks for Semantic Segmentation 阅读笔记</div></div></a></div><div><a href="/2022/05/06/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/Enhancing%20photorealism%20enhancement/" title="Enhancing photorealism enhancement"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.imgdb.cn/item/61ed14292ab3f51d911d280f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-06</div><div class="title">Enhancing photorealism enhancement</div></div></a></div><div><a href="/2022/03/31/%E5%87%BA%E5%9B%BD/%E9%A1%B9%E8%84%8A%E8%BD%A9%E5%BF%97/" title="临别项脊轩"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.imgdb.cn/item/61eccb4d2ab3f51d91d616f7.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-31</div><div class="title">临别项脊轩</div></div></a></div><div><a href="/2022/05/25/%E9%9A%8F%E7%AC%94/%E6%B8%85%E5%8D%8E%E5%9B%AD%E6%97%A5%E8%AE%B0/%E6%B8%85%E5%8D%8E%E5%9B%AD%E6%97%A5%E8%AE%B0--%E7%AC%AC%E4%BA%94%E9%83%A8/" title="清华园日记——第五部"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://zhaochenyang20.github.io/pic/embed/5_31_1.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-25</div><div class="title">清华园日记——第五部</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#segement-me-if-u-can"><span class="toc-number">1.</span> <span class="toc-text">Segement Me If U Can</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1%E7%AE%80%E4%BB%8B"><span class="toc-number">2.</span> <span class="toc-text">任务简介</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.</span> <span class="toc-text">实验数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E8%A6%81%E6%B1%82"><span class="toc-number">2.2.</span> <span class="toc-text">实验要求</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="toc-number">2.3.</span> <span class="toc-text">评价指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%A5%E5%91%8A%E5%86%85%E5%AE%B9"><span class="toc-number">2.4.</span> <span class="toc-text">报告内容</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#question-list"><span class="toc-number">2.5.</span> <span class="toc-text">Question List</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">3.</span> <span class="toc-text">模型结构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#bi-lstm"><span class="toc-number">3.1.</span> <span class="toc-text">Bi-LSTM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#text-cnn"><span class="toc-number">3.2.</span> <span class="toc-text">Text-CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#mlp"><span class="toc-number">3.3.</span> <span class="toc-text">MLP</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF"><span class="toc-number">4.</span> <span class="toc-text">配置信息</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%BA%93"><span class="toc-number">4.1.</span> <span class="toc-text">环境库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-number">4.2.</span> <span class="toc-text">可视化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E5%8A%9B"><span class="toc-number">4.3.</span> <span class="toc-text">算力</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">5.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94"><span class="toc-number">5.1.</span> <span class="toc-text">模型对比</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#accuracy"><span class="toc-number">5.1.1.</span> <span class="toc-text">accuracy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#f1_score"><span class="toc-number">5.1.2.</span> <span class="toc-text">f1_score</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loss"><span class="toc-number">5.1.3.</span> <span class="toc-text">loss</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%AF%B9%E6%AF%94"><span class="toc-number">5.2.</span> <span class="toc-text">参数对比</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#max_length"><span class="toc-number">5.2.1.</span> <span class="toc-text">max_length</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="toc-number">5.2.2.</span> <span class="toc-text">初始学习率</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E6%80%9D%E8%80%83"><span class="toc-number">6.</span> <span class="toc-text">问题思考</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E5%81%9C%E6%AD%A2"><span class="toc-number">6.1.</span> <span class="toc-text">实验停止</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">6.2.</span> <span class="toc-text">参数初始化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98"><span class="toc-number">6.3.</span> <span class="toc-text">过拟合问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AF%B9%E6%AF%94-1"><span class="toc-number">6.4.</span> <span class="toc-text">模型对比</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B3%E4%BA%8E-bert"><span class="toc-number">7.</span> <span class="toc-text">关于 Bert</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BF%83%E5%BE%97%E4%BD%93%E4%BC%9A"><span class="toc-number">8.</span> <span class="toc-text">心得体会</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background: ＃0096FF"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Eren Zhao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script type="text/javascript" id="maid-script" src="https://unpkg.com/mermaid@undefined/dist/mermaid.min.js?v=undefined"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库正在艰难运行</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '978121c7b834efdd76be',
      clientSecret: '59b40e8f39a1c33db5a2c891771086164b9575c4',
      repo: 'zhaochenyang20.github.io',
      owner: 'zhaochenyang20',
      admin: ['zhaochenyang20'],
      id: '744d254a2c29d52c992bd146f7e900b0',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>