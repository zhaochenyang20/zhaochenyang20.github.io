<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Vision Transformer Learning Log | 求道之人，不问寒暑</title><meta name="keywords" content="2022寒假,科研"><meta name="author" content="Eren Zhao"><meta name="copyright" content="Eren Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="每次都觉得狗家的论文特别阔气...">
<meta property="og:type" content="article">
<meta property="og:title" content="Vision Transformer Learning Log">
<meta property="og:url" content="http://example.com/2022/01/10/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/VIT/index.html">
<meta property="og:site_name" content="求道之人，不问寒暑">
<meta property="og:description" content="每次都觉得狗家的论文特别阔气...">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Casey Horner (rtCujH697DU).jpg">
<meta property="article:published_time" content="2022-01-10T08:20:49.630Z">
<meta property="article:modified_time" content="2022-08-26T15:50:09.473Z">
<meta property="article:author" content="Eren Zhao">
<meta property="article:tag" content="2022寒假">
<meta property="article:tag" content="科研">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Casey Horner (rtCujH697DU).jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://example.com/2022/01/10/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/VIT/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"麻了，找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Vision Transformer Learning Log',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-08-26 23:50:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><meta name="generator" content="Hexo 6.0.0"><link rel="alternate" href="/atom.xml" title="求道之人，不问寒暑" type="application/atom+xml">
<style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}</style></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend.jpg'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">192</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">52</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">31</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/list-100/"><i class="fa-fw fas fa-music"></i><span> TODO</span></a></div><div class="menus_item"><a class="site-page" href="/projects/"><i class="fa-fw fas fa-video"></i><span> Projects</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Casey Horner (rtCujH697DU).jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">求道之人，不问寒暑</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/list-100/"><i class="fa-fw fas fa-music"></i><span> TODO</span></a></div><div class="menus_item"><a class="site-page" href="/projects/"><i class="fa-fw fas fa-video"></i><span> Projects</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Vision Transformer Learning Log</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-01-10T08:20:49.630Z" title="发表于 2022-01-10 16:20:49">2022-01-10</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-08-26T15:50:09.473Z" title="更新于 2022-08-26 23:50:09">2022-08-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/">科研</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/CV/">CV</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">2.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>9分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Vision Transformer Learning Log"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><img src="https://s2.loli.net/2022/01/10/GQ3poJkgA6sryzD.jpg" alt="Photo by Alistair MacKenzie (WvM3RQSElRc).jpg"></p>
<h1 id="从Attenion-Is-All-You-Need到An-Image-is-Worth-16x16-Words"><a href="#从Attenion-Is-All-You-Need到An-Image-is-Worth-16x16-Words" class="headerlink" title="从Attenion Is All You Need到An Image is Worth 16x16 Words"></a>从Attenion Is All You Need到An Image is Worth 16x16 Words</h1><h2 id="Why-“Attenion-is-all-you-need”"><a href="#Why-“Attenion-is-all-you-need”" class="headerlink" title="Why “Attenion is all you need”?"></a>Why “Attenion is all you need”?</h2><h3 id="RNN与CNN的劣势"><a href="#RNN与CNN的劣势" class="headerlink" title="RNN与CNN的劣势"></a><code>RNN</code>与<code>CNN</code>的劣势</h3><p><code>RNN</code>很不容易并行化 <code>(hard to parallel）</code></p>
<p><code>CNN</code>可以并行化，但是<code>CNN</code>只能考虑非常有限的内容。如果要考虑长时间的<code>dependency</code>，需要堆叠<code>filter</code>。堆叠越深，上层的<code>filter</code>就可以考虑越多的资讯。然而，堆叠很多层才能看到较为长时的资讯。（怀疑原文是台湾同胞写的，特别喜欢用资讯一个词）</p>
<h3 id="self-attention的作用"><a href="#self-attention的作用" class="headerlink" title="self-attention的作用"></a><code>self-attention</code>的作用</h3><p> 概括言之：并行化且全局化</p>
<p>具体实现在<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/340149804">知乎</a>上写的很清楚，还可以参考<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Wv411h7kN?p=34">李宏毅的机器学习</a>。李宏毅的机器学习推荐从<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Wv411h7kN?p=22"><code>CNN</code></a></p>
<p>开始，听<code>P22、P23、P24、P34、P35</code>。</p>
<h3 id="multi-head-self-atteniton"><a href="#multi-head-self-atteniton" class="headerlink" title="multi-head self-atteniton"></a><code>multi-head self-atteniton</code></h3><p>概括而言就是基于<code>q</code>转移出更多的转移向量。其作用类似于<code>CNN</code>里面的<code>multi-channel</code>，卷积核更多后能够监视的咨询更多。<code>head</code>更多后能够监视的内容也更多。</p>
<h3 id="postion-encoding"><a href="#postion-encoding" class="headerlink" title="postion-encoding"></a><code>postion-encoding</code></h3><p><code>RNN</code>虽然丢失了并行性，然而由于其运行逻辑几乎就是语义逻辑，天然就涵盖了位置信息。然而<code>self-attention</code>由于其高并行度，反而丧失了位置信息。比如A的年龄比B大，如果不考虑位置信息，对于<code>attention</code>而言，和B的年龄比A大毫无区别。(实际上不考虑位置信息，则这个句子会被<code>attenion</code>完全乱序理解，理解为A龄B年比大也没有问题)</p>
<p>所以<code>attention</code>需要有位置信息来保证序列性。</p>
<h3 id="attention和CNN的关系"><a href="#attention和CNN的关系" class="headerlink" title="attention和CNN的关系"></a><code>attention</code>和<code>CNN</code>的关系</h3><p><code>self-attention</code>是更广义的<code>CNN</code>，则这个模型更加<code>flexible</code>。而我们认为，一个模型越<code>flexible</code>，训练它所需要的数据量就越多，所以在训练<code>self-attention</code>模型时就需要更多的数据。</p>
<p>这一点在<code>ViT</code>中更明显，狗家为了训他，用了<code>3</code>亿张图片的<code>JFT-300</code>，而如果不使用这么多数据而只使用<code>ImageNet</code>，则性能不如<code>CNN</code>。侧面体现每次狗家的论文都特别阔气。</p>
<h1 id="End-to-End-Object-Detection-with-Transformers"><a href="#End-to-End-Object-Detection-with-Transformers" class="headerlink" title="End-to-End Object Detection with Transformers"></a>End-to-End Object Detection with Transformers</h1><h2 id="文章概述"><a href="#文章概述" class="headerlink" title="文章概述"></a>文章概述</h2><h3 id="论文试图解决的问题"><a href="#论文试图解决的问题" class="headerlink" title="论文试图解决的问题"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/438770010">论文试图解决的问题</a></h3><p>文章把目标检测看做是一种<code>set prediction</code>的问题，通过引入<code>transformer</code>且设置了一个独特的<code>set prediction loss</code>，解决了传统的检测方法中需要很多手工设计<code>hand-designed</code>的模块：诸如非极大值抑制，<code>anchor</code>的设计等等。经过<code>COCO</code>数据集测试，新算法在速度和精度上都比<code>Faster-RCNN</code>高。</p>
<h3 id="文章验证的科学假设"><a href="#文章验证的科学假设" class="headerlink" title="文章验证的科学假设"></a>文章验证的科学假设</h3><p>能否通过引入<code>transformer</code>和二分匹配的策略计算<code>loss</code>，去优化掉传统检测方法中的<code>hand-designed</code>模块。</p>
<h3 id="解决方案的关键"><a href="#解决方案的关键" class="headerlink" title="解决方案的关键"></a>解决方案的关键</h3><p>基于匈牙利算法的<code>set prediction loss</code></p>
<h2 id="原理解读"><a href="#原理解读" class="headerlink" title="原理解读"></a>原理解读</h2><p>文章所做的工作，就是将<code>transformers</code>运用到了<code>object detection</code>领域，并且取得了不错的结果。</p>
<p>一次预测，端到端训练，<code>set loss function</code>和二分匹配</p>
<p>第一个是用<code>transformer</code>的<code>encoder-decoder</code>架构一次性生成<code>N</code>个<code>box prediction</code>。其中<code>N</code>是一个事先设定的、远远大于<code>image</code>中<code>object</code>个数的一个整数。</p>
<p>第二个是设计了<code>bipartite matching loss</code>，基于预测的<code>boxex</code>和<code>ground truth boxes</code>的二分图匹配计算<code>loss</code>的大小，从而使得预测的<code>box</code>的位置和类别更接近于<code>ground truth</code>。</p>
<p><code>DETR</code>整体结构可以分为四个部分：<code>backbone，encoder，decoder</code>和<code>FFN</code></p>
<p><img src="https://s2.loli.net/2022/01/11/GrEiMJgLR8Z4NzK.png" alt="image.png"></p>
<h3 id="backbone"><a href="#backbone" class="headerlink" title="backbone"></a><code>backbone</code></h3><p>将图像转换为<code>feature map</code></p>
<h3 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a><code>encoder</code></h3><p>通道压缩，转化为序列化数据，位置编码</p>
<h4 id="与初始的encoder区别"><a href="#与初始的encoder区别" class="headerlink" title="与初始的encoder区别"></a>与初始的<code>encoder</code>区别</h4><ul>
<li>输入编码器的位置编码需要考虑<code>2-D</code>空间位置。</li>
<li>位置编码向量需要加入到每个<code>Encoder Layer</code>中。</li>
<li>在编码器内部位置编码<code>Positional Encoding</code>仅仅作用于<code>Query</code>和<code>Key</code>，即只与<code>Query</code>和<code>Key</code>相加，<code>Value</code>不做任何处理。</li>
</ul>
<h3 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a><code>decoder</code></h3><p><code>DETR</code>的<code>Transformer Decoder</code>是一次性处理全部的<code>object queries</code>，即一次性输出全部的<code>predictions</code>；而不像原始的<code>Transformer</code>是<code>auto-regressive</code>的，从左到右一个词一个词地输出。这个过程我们表达为：<code>decodes the N objects in parallel at each decoder layer</code></p>
<h4 id="DETR的Decoder主要有两个输入"><a href="#DETR的Decoder主要有两个输入" class="headerlink" title="DETR的Decoder主要有两个输入"></a><code>DETR</code>的<code>Decoder</code>主要有两个输入</h4><ol>
<li><code>Transformer Encoder</code>输出的<code>Embedding</code>与 <code>position encoding</code>之和</li>
<li><code>Object queries</code></li>
</ol>
<h4 id="DETR是怎么训练"><a href="#DETR是怎么训练" class="headerlink" title="DETR是怎么训练"></a><code>DETR</code>是怎么训练</h4><p>我们用<code>scipy.optimize</code>这个库中的 <code>linear_sum_assignment</code>函数找到最优的匹配，这个过程我们称之为匈牙利算法<code>Hungarian Algorithm</code>，即根据两个集合之间的连接权重去寻找集合间的最优匹配，连接权重大的优先匹配。根据匈牙利算法得出的预测值，计算<code>loss</code>。</p>
<p>简单来说，就是本文固定输出为<code>N=100</code>个预测目标，然后与之相应的<code>Groundtruth</code>也会使用空标记来进行补齐，达到<code>100</code>个。然后双方数目一样后，就可以用匈牙利算法进行二部图匹配，将预测框逐个分配给最优匹配的<code>groundtruth</code>框。</p>
<h1 id="Deformable-DETR-Deformable-Transformer-For-End-To-End-Object-Detection"><a href="#Deformable-DETR-Deformable-Transformer-For-End-To-End-Object-Detection" class="headerlink" title="Deformable DETR: Deformable Transformer For End-To-End Object Detection"></a>Deformable DETR: Deformable Transformer For End-To-End Object Detection</h1><h2 id="初代DETR"><a href="#初代DETR" class="headerlink" title="初代DETR"></a>初代<code>DETR</code></h2><h3 id="Deformable-Convolution的意义"><a href="#Deformable-Convolution的意义" class="headerlink" title="Deformable Convolution的意义"></a><code>Deformable Convolution</code>的意义</h3><p><code>DETR</code>即一种全新的卷积结构。这种方法将固定形状的卷积过程改造成了能适应物体形状的可变的卷积过程，从而使结构适应物体形变的能力更强。</p>
<p>传统的<code>CNN</code>只能靠一些简单的方法(比如<code>max pooling</code>)来适应物体的形变，如果形变的太厉害就无能为力了。因为<code>CNN</code>的卷积核的<code>geometric structure</code>是固定的——卷积核总是在固定位置对输入特征特征进行采样。</p>
<h3 id="传统解决物体形变的方式"><a href="#传统解决物体形变的方式" class="headerlink" title="传统解决物体形变的方式"></a>传统解决物体形变的方式</h3><ol>
<li><p>使用大量的数据进行训练。比如用<code>ImageNet</code>数据集，再在其基础上做翻转等变化来扩展数据集，通俗地说就是通过穷举的方法使模型能够适应各种形状的物体，这种方法收敛较慢而且要设计复杂的网络结构才能达到理想的结果。</p>
</li>
<li><p>设计一些特殊的算法来适应形变。比如<code>SIFT</code>，目标检测时用滑动窗口法来适应目标在不同位置上的分类也属于这类。</p>
</li>
</ol>
<p>对第一种方法，如果用训练中没有遇到过的新形状物体(但同属于一类)来做测试，由于新形状没有训练过，会造成测试不准确，而且靠数据集来适应形变的训练过程太耗时，网络结构也必须设计的很复杂。</p>
<p>对于第二种方法，如果物体的形状极其复复杂，要设计出能适应这种复杂结构的算法就更困难了。</p>
<h3 id="方法提出"><a href="#方法提出" class="headerlink" title="方法提出"></a>方法提出</h3><p>为了解决<code>CNN</code>的卷积核的<code>geometric structure</code>是<code>fixed</code>的问题，代季峰老师等人提出<code>Deformable Convolution</code>方法，它对感受野上的每一个点加一个偏移量，偏移的大小是通过学习得来的，偏移后感受野不再是个正方形，而是和物体的实际形状相匹配。这么做的好处就是无论物体怎么形变，卷积的区域始终覆盖在物体形状的周围。</p>
<p><img src="https://pic1.zhimg.com/v2-979535a9858b3e61b958f81d88262ff0_r.jpg" alt=""></p>
<h3 id="方法意义"><a href="#方法意义" class="headerlink" title="方法意义"></a>方法意义</h3><p>作者认为可形变卷积的优势还是很大的</p>
<ul>
<li>对物体的形变和尺度建模的能力比较强。</li>
<li>感受野比一般卷积大很多，因为有偏移的原因，实际上相关实验已经表明了DNN网络很多时候受感受野不足的条件制约；但是对于一般的空洞而言，卷积空洞是固定的，对不同的数据集不同情况可能最适合的空洞大小是不同的，但是可形变卷积的偏移是可以根据具体数据的情况进行学习的。</li>
</ul>
<h3 id="初代DETR方法缺陷"><a href="#初代DETR方法缺陷" class="headerlink" title="初代DETR方法缺陷"></a>初代<code>DETR</code>方法缺陷</h3><h4 id="训练时间长"><a href="#训练时间长" class="headerlink" title="训练时间长"></a>训练时间长</h4><p>相比于已有的检测器，DETR需要更久的训练才能达到收敛<code>500 epochs</code>，比<code>Faster R-CNN</code>慢了<code>10-20</code>倍。</p>
<h4 id="计算复杂度高"><a href="#计算复杂度高" class="headerlink" title="计算复杂度高"></a>计算复杂度高</h4><p>发现<code>DETR</code>对小目标的性能很差，现代许多种检测器通常利用多尺度特征，从高分辨率<code>High Resolution</code>的特征图中检测小物体。但是高分辨率的特征图会大大提高<code>DETR</code>复杂度。</p>
<h2 id="Deformable-DETR"><a href="#Deformable-DETR" class="headerlink" title="Deformable DETR"></a><code>Deformable DETR</code></h2><p><code>Deformable DETR</code>利用了可变形卷积<code>Deformable Convolution</code>的稀疏空间采样的本领，以及<code>Transformer</code>对于相关性建模的能力，针对此提出了一种<code>Deformable Attention Module</code>，只关注一个<code>feature map</code>中的一小部分关键的位置，起到<code>pre-filter</code>的作用，可以自然地结合上<code>FPN</code>，聚集多尺度特征。作者使用它来替换<code>Transformer</code>的<code>attention moudle</code>。</p>
<p>设计初衷为：传统的<code>attention</code>的每个<code>query</code>都会和所有的<code>key</code>做<code>attention</code>，而<code>Deformable Attention</code>只是用固定一部分<code>key</code>和<code>query</code>去做<code>attention</code>，降低了收敛时间。</p>
<h1 id="An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><a href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale" class="headerlink" title="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"></a>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</h1><p><del>因为还是<code>ICLR 2021 under review</code>，所以作者目前还是匿名的，但是看其实验用到的<code>TPU</code>，能够大概猜出应该是<code>Google</code>爸爸的文章（看着实验的配置，不得不感慨钞能力的力量）。</del></p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>近年来，<code>Transformer</code>已经成了<code>NLP</code>领域的标准配置，但是<code>CV</code>领域还是<code>CNN</code>（如<code>ResNet, DenseNet</code>等）占据了绝大多数的<code>SOTA</code>结果。</p>
<p>最近<code>CV</code>界也有很多文章将<code>transformer</code>迁移到<code>CV</code>领域，这些文章总的来说可以分为两个大类：</p>
<ol>
<li>将<code>self-attention</code>机制与常见的<code>CNN</code>架构结合；</li>
<li>用<code>self-attention</code>机制完全替代<code>CNN</code>。</li>
</ol>
<p>本文采用的也是第<code>2</code>种思路。虽然已经有很多工作用<code>self-attention</code>完全替代<code>CNN</code>，且在理论上效率比较高，但是它们用了特殊的<code>attention</code>机制，无法从硬件层面加速，所以目前<code>CV</code>领域的<code>SOTA</code>结果还是被<code>CNN</code>架构所占据。</p>
<p>文章不同于以往工作的地方，就是尽可能地将<code>NLP</code>领域的<code>transformer</code>不作修改地(开盒即用地)搬到<code>CV</code>领域来(<code>ViT</code>其实只用到了<code>Transformer</code>的<code>Encoder</code>，而并没有用到<code>Decoder</code>)</p>
<p>本着尽可能少修改的原则，作者将原版的<code>Transformer</code>开箱即用地迁移到分类任务上面。并且作者认为没有必要总是依赖于<code>CNN</code>，只用<code>Transformer</code>也能够在分类任务中表现很好，尤其是在使用大规模训练集的时候。同时，在大规模数据集上预训练好的模型，在迁移到中等数据集或小数据集的分类任务上以后，也能取得比<code>CNN</code>更优的性能。</p>
<h2 id="具体方法"><a href="#具体方法" class="headerlink" title="具体方法"></a>具体方法</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/266311690">来自此处链接</a></p>
<h3 id="图片预处理：分块和降维"><a href="#图片预处理：分块和降维" class="headerlink" title="图片预处理：分块和降维"></a>图片预处理：分块和降维</h3><p>将图像转化为序列化数据，首先将图像分割成一个个<code>patch</code>，然后将每个<code>patch reshape</code>成一个向量，得到所谓的<code>flattened patch</code>。</p>
<h3 id="Patch-Embedding"><a href="#Patch-Embedding" class="headerlink" title="Patch Embedding"></a>Patch Embedding</h3><p>对每个向量都做一个线性变换(即全连接层)，这里我们称其为<code>Patch Embedding</code>。</p>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>由于<code>transformer</code>模型本身是没有位置信息的，和<code>NLP</code>中一样，我们需要用<code>position embedding</code>将位置信息加到模型中去。</p>
<h3 id="MLP-Classification-Head"><a href="#MLP-Classification-Head" class="headerlink" title="MLP Classification Head"></a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E5%99%A8">MLP Classification Head</a></h3><p><img src="https://pic2.zhimg.com/80/v2-7439a17c2e9aa981c95d783a93cb8729_1440w.jpg" style="zoom:50%;" /></p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>我们发现： 当在最小数据集<code>ImageNet</code>上进行预训练时，尽管进行了大量的正则化等操作，但<code>ViT</code>大模型的性能不如<code>ViT-Base</code>模型。</p>
<p>稍微大一点的<code>ImageNet-21k</code>预训练，它们的表现也差不多。</p>
<p>只有到了<code>JFT 300M</code>，我们才能看到更大的<code>ViT</code>模型全部优势。在更大的数据集上，<code>ViT</code>超过了所有的模型，取得了<code>SOTA</code>。</p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/2022%E5%AF%92%E5%81%87/">2022寒假</a><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-62b11572b25ab3ab" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/01/10/Lecture/%E4%BB%8E%E5%89%8D%E7%9A%84%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/OOP/oop_tutorial_readme/"><img class="prev-cover" src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Ignacio Giri (Furrz7uDeaA).jpg" onerror="onerror=null;src='https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Oscar Rodrigo Hernandez Panczenko (P0GQ95huhBo).jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">OOP笔记 Readme</div></div></a></div><div class="next-post pull-right"><a href="/2022/01/10/CS/%E5%85%A8%E6%A0%88%E5%BC%80%E5%8F%91/css/"><img class="next-cover" src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Francesco Ungaro (CKoBHPy9tmo).jpg" onerror="onerror=null;src='https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Oscar Rodrigo Hernandez Panczenko (P0GQ95huhBo).jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">web前端工程狮 CSS</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/01/13/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/%E8%B0%83%E5%8F%82/" title="人生第一次调参"><img class="cover" src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Darrell Cassell (hoCXpPUMCoE).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-13</div><div class="title">人生第一次调参</div></div></a></div><div><a href="/2022/01/11/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/paper_reading/" title="如何有效的读一篇论文"><img class="cover" src="https://pic.imgdb.cn/item/61eccb202ab3f51d91d5f178.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-11</div><div class="title">如何有效的读一篇论文</div></div></a></div><div><a href="/2022/01/12/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/FPN/" title="FPN & RefineNet"><img class="cover" src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Ethan Brooke (M2VtwQSkQFs).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-12</div><div class="title">FPN & RefineNet</div></div></a></div><div><a href="/2022/01/20/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/equivariant/" title="Euivariant & Invariant"><img class="cover" src="https://pic.imgdb.cn/item/61f106862ab3f51d917b202b.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-20</div><div class="title">Euivariant & Invariant</div></div></a></div><div><a href="/2022/01/15/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4/Dive%20Into%20Deep%20Learning%20Part%201/" title="Dive Into Deep Learning Chapter 1~3"><img class="cover" src="https://pic.imgdb.cn/item/61f106b42ab3f51d917b5221.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-15</div><div class="title">Dive Into Deep Learning Chapter 1~3</div></div></a></div><div><a href="/2022/02/16/CS/others/python_decorator/" title="decorator Python装饰器"><img class="cover" src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Joshua Case (hZTdj71x_o0).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-16</div><div class="title">decorator Python装饰器</div></div></a></div><div><a href="/2022/02/03/%E5%87%BA%E5%9B%BD/Night%20Voyager/" title="Night Voyager"><img class="cover" src="https://pic.imgdb.cn/item/61fd0f4e2ab3f51d91892452.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-03</div><div class="title">Night Voyager</div></div></a></div><div><a href="/2022/02/08/%E9%9A%8F%E7%AC%94/%E6%80%BB%E7%BB%93/Say%20Something%20for%20Spring%202022/" title="Say Something for Spring 2022"><img class="cover" src="https://pic.imgdb.cn/item/620540f92ab3f51d91b65009.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-08</div><div class="title">Say Something for Spring 2022</div></div></a></div><div><a href="/2022/03/17/Lecture/%E4%BB%8E%E5%89%8D%E7%9A%84%E6%95%B0%E5%AD%A6/Gauss%E5%85%AC%E5%BC%8F%E5%92%8CStokes%E5%85%AC%E5%BC%8F/" title="多元微积分 Gauss 公式与 Stokes 公式总结"><img class="cover" src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Lyndon Li (zrT1tjnxJKQ).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-17</div><div class="title">多元微积分 Gauss 公式与 Stokes 公式总结</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%8EAttenion-Is-All-You-Need%E5%88%B0An-Image-is-Worth-16x16-Words"><span class="toc-number">1.</span> <span class="toc-text">从Attenion Is All You Need到An Image is Worth 16x16 Words</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-%E2%80%9CAttenion-is-all-you-need%E2%80%9D"><span class="toc-number">1.1.</span> <span class="toc-text">Why “Attenion is all you need”?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RNN%E4%B8%8ECNN%E7%9A%84%E5%8A%A3%E5%8A%BF"><span class="toc-number">1.1.1.</span> <span class="toc-text">RNN与CNN的劣势</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#self-attention%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">1.1.2.</span> <span class="toc-text">self-attention的作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-self-atteniton"><span class="toc-number">1.1.3.</span> <span class="toc-text">multi-head self-atteniton</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#postion-encoding"><span class="toc-number">1.1.4.</span> <span class="toc-text">postion-encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention%E5%92%8CCNN%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">1.1.5.</span> <span class="toc-text">attention和CNN的关系</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#End-to-End-Object-Detection-with-Transformers"><span class="toc-number">2.</span> <span class="toc-text">End-to-End Object Detection with Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E7%AB%A0%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">文章概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E8%AF%95%E5%9B%BE%E8%A7%A3%E5%86%B3%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">2.1.1.</span> <span class="toc-text">论文试图解决的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E7%AB%A0%E9%AA%8C%E8%AF%81%E7%9A%84%E7%A7%91%E5%AD%A6%E5%81%87%E8%AE%BE"><span class="toc-number">2.1.2.</span> <span class="toc-text">文章验证的科学假设</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%E7%9A%84%E5%85%B3%E9%94%AE"><span class="toc-number">2.1.3.</span> <span class="toc-text">解决方案的关键</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86%E8%A7%A3%E8%AF%BB"><span class="toc-number">2.2.</span> <span class="toc-text">原理解读</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#backbone"><span class="toc-number">2.2.1.</span> <span class="toc-text">backbone</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder"><span class="toc-number">2.2.2.</span> <span class="toc-text">encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8E%E5%88%9D%E5%A7%8B%E7%9A%84encoder%E5%8C%BA%E5%88%AB"><span class="toc-number">2.2.2.1.</span> <span class="toc-text">与初始的encoder区别</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder"><span class="toc-number">2.2.3.</span> <span class="toc-text">decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DETR%E7%9A%84Decoder%E4%B8%BB%E8%A6%81%E6%9C%89%E4%B8%A4%E4%B8%AA%E8%BE%93%E5%85%A5"><span class="toc-number">2.2.3.1.</span> <span class="toc-text">DETR的Decoder主要有两个输入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DETR%E6%98%AF%E6%80%8E%E4%B9%88%E8%AE%AD%E7%BB%83"><span class="toc-number">2.2.3.2.</span> <span class="toc-text">DETR是怎么训练</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Deformable-DETR-Deformable-Transformer-For-End-To-End-Object-Detection"><span class="toc-number">3.</span> <span class="toc-text">Deformable DETR: Deformable Transformer For End-To-End Object Detection</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E4%BB%A3DETR"><span class="toc-number">3.1.</span> <span class="toc-text">初代DETR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Deformable-Convolution%E7%9A%84%E6%84%8F%E4%B9%89"><span class="toc-number">3.1.1.</span> <span class="toc-text">Deformable Convolution的意义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E8%A7%A3%E5%86%B3%E7%89%A9%E4%BD%93%E5%BD%A2%E5%8F%98%E7%9A%84%E6%96%B9%E5%BC%8F"><span class="toc-number">3.1.2.</span> <span class="toc-text">传统解决物体形变的方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E6%8F%90%E5%87%BA"><span class="toc-number">3.1.3.</span> <span class="toc-text">方法提出</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E6%B3%95%E6%84%8F%E4%B9%89"><span class="toc-number">3.1.4.</span> <span class="toc-text">方法意义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E4%BB%A3DETR%E6%96%B9%E6%B3%95%E7%BC%BA%E9%99%B7"><span class="toc-number">3.1.5.</span> <span class="toc-text">初代DETR方法缺陷</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%97%B6%E9%97%B4%E9%95%BF"><span class="toc-number">3.1.5.1.</span> <span class="toc-text">训练时间长</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E9%AB%98"><span class="toc-number">3.1.5.2.</span> <span class="toc-text">计算复杂度高</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deformable-DETR"><span class="toc-number">3.2.</span> <span class="toc-text">Deformable DETR</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale"><span class="toc-number">4.</span> <span class="toc-text">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">4.1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">具体方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E9%A2%84%E5%A4%84%E7%90%86%EF%BC%9A%E5%88%86%E5%9D%97%E5%92%8C%E9%99%8D%E7%BB%B4"><span class="toc-number">4.2.1.</span> <span class="toc-text">图片预处理：分块和降维</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Patch-Embedding"><span class="toc-number">4.2.2.</span> <span class="toc-text">Patch Embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Positional-Encoding"><span class="toc-number">4.2.3.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MLP-Classification-Head"><span class="toc-number">4.2.4.</span> <span class="toc-text">MLP Classification Head</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">4.3.</span> <span class="toc-text">实验结果</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: ＃0096FF"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Eren Zhao</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script type="text/javascript" id="maid-script" src="https://unpkg.com/mermaid@undefined/dist/mermaid.min.js?v=undefined"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库正在艰难运行</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '978121c7b834efdd76be',
      clientSecret: '59b40e8f39a1c33db5a2c891771086164b9575c4',
      repo: 'zhaochenyang20.github.io',
      owner: 'zhaochenyang20',
      admin: ['zhaochenyang20'],
      id: '2aeb87c8bdd38c9a2dd389f0c41dc686',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>