<!DOCTYPE html><html lang="en-US" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Dive Into Deep Learning Chapter 1~3 | 求道之人，不问寒暑</title><meta name="keywords" content="科研,2022寒假,深度学习,DIDL"><meta name="author" content="Eren Zhao"><meta name="copyright" content="Eren Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="本着不嫖白不嫖的想法，反正还有人带我学习《带你学习深度学习》....">
<meta property="og:type" content="article">
<meta property="og:title" content="Dive Into Deep Learning Chapter 1~3">
<meta property="og:url" content="http://example.com/2022/01/15/%E7%A3%95%E7%9B%90/Dive%20Into%20Deep%20Learning%20Part%201/index.html">
<meta property="og:site_name" content="求道之人，不问寒暑">
<meta property="og:description" content="本着不嫖白不嫖的想法，反正还有人带我学习《带你学习深度学习》....">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pic.imgdb.cn/item/61f0fc6b2ab3f51d91730400.jpg">
<meta property="article:published_time" content="2022-01-15T01:47:21.196Z">
<meta property="article:modified_time" content="2022-04-02T08:42:40.072Z">
<meta property="article:author" content="Eren Zhao">
<meta property="article:tag" content="科研">
<meta property="article:tag" content="2022寒假">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="DIDL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.imgdb.cn/item/61f0fc6b2ab3f51d91730400.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://example.com/2022/01/15/%E7%A3%95%E7%9B%90/Dive%20Into%20Deep%20Learning%20Part%201/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Dive Into Deep Learning Chapter 1~3',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-04-02 16:42:40'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.0.0"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}</style></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend.jpg'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">166</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">53</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.imgdb.cn/item/61f0fc6b2ab3f51d91730400.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">求道之人，不问寒暑</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Dive Into Deep Learning Chapter 1~3</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-01-15T01:47:21.196Z" title="Created 2022-01-15 09:47:21">2022-01-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2022-04-02T08:42:40.072Z" title="Updated 2022-04-02 16:42:40">2022-04-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/">科研</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">12.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>48min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Dive Into Deep Learning Chapter 1~3"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="学习要求">学习要求</h1>
<ol type="1">
<li>《动手学深度学习》<code>1,2,3,5,9</code>章节(主要涉及深度学习基础、计算机视觉基础)</li>
<li><a target="_blank" rel="noopener" href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">书本网址</a></li>
<li>科协提供答疑服务，<a target="_blank" rel="noopener" href="https://docs.qq.com/doc/DV1N1eGtZUUdWVmRX">答疑文档</a></li>
<li>科协提供课后作业，完成作业且达到指定性能可以获得价值百元的自选物品。<a target="_blank" rel="noopener" href="https://cloud.tsinghua.edu.cn/d/491de36021944d6bb289/">指定项目网址</a></li>
<li><code>test accuracy</code>达到<code>68%</code>及以上，需提交能证明性能的文件，包括但不限于: 训练曲线截图、模型、训练代码等</li>
</ol>
<p><img src="https://tangshusen.me/Dive-into-DL-PyTorch/img/book-org.svg" /></p>
<h1 id="intro">intro</h1>
<p>我们很容易就能找到一些连世界上最好的程序员也无法仅用编程技巧解决的简单问题。例如，假设我们想要编写一个判定一张图像中有没有猫的程序。这件事听起来好像很简单，对不对？程序只需要对每张输入图像输出“真”（表示有猫）或者“假”（表示无猫）即可。但令人惊讶的是，即使是世界上最优秀的计算机科学家和程序员也不懂如何编写这样的程序。</p>
<p>我们该从哪里入手呢？我们先进一步简化这个问题：若假设所有图像的高和宽都是同样的400像素大小，一个像素由红绿蓝三个值构成，那么一张图像就由近50万个数值表示。那么哪些数值隐藏着我们需要的信息呢？是所有数值的平均数，还是四个角的数值，抑或是图像中的某一个特别的点？事实上，要想解读图像中的内容，需要寻找仅仅在结合成千上万的数值时才会出现的特征，如边缘、质地、形状、眼睛、鼻子等，最终才能判断图像中是否有猫。</p>
<h2 id="用数据编程">用数据编程</h2>
<p>一种解决以上问题的思路是逆向思考。与其设计一个解决问题的程序，不如从最终的需求入手来寻找一个解决方案。事实上，这也是目前的机器学习和深度学习应用共同的核心思想：我们可以称其为“用数据编程”。与其枯坐在房间里思考怎么设计一个识别猫的程序，不如利用人类肉眼在图像中识别猫的能力。我们可以收集一些已知包含猫与不包含猫的真实图像，然后我们的目标就转化成如何从这些图像入手得到一个可以推断出图像中是否有猫的函数。这个函数的形式通常通过我们的知识来针对特定问题选定。例如，我们使用一个二次函数来判断图像中是否有猫，但是像二次函数系数值这样的函数参数的具体值则是通过数据来确定。</p>
<h2 id="深度学习">深度学习</h2>
<p>通俗来说，机器学习是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。深度学习是指机器学习中的一类函数，它们的形式通常为多层神经网络。近年来，仰仗着大数据集和强大的硬件，深度学习已逐渐成为处理图像、文本语料和声音信号等复杂高维度数据的主要方法。</p>
<p>我们现在正处于一个程序设计得到深度学习的帮助越来越多的时代。这可以说是计算机科学历史上的一个分水岭。举个例子，深度学习已经在你的手机里：拼写校正、语音识别、认出社交媒体照片里的好友们等。得益于优秀的算法、快速而廉价的算力、前所未有的大量数据以及强大的软件工具，如今大多数软件工程师都有能力建立复杂的模型来解决十年前连最优秀的科学家都觉得棘手的问题。</p>
<h1 id="起源">起源</h1>
<p>神经网络的生物学解释被稀释，但仍保留了这个名字。时至今日，绝大多数神经网络都包含以下的核心原则。</p>
<ul>
<li>交替使用线性处理单元与非线性处理单元，它们经常被称为“层”。</li>
<li>使用链式法则（即反向传播）来更新网络的参数。</li>
</ul>
<h1 id="发展">发展</h1>
<p>存储容量没能跟上数据量增长的步伐，与此同时，计算力的增长又盖过了数据量的增长。这样的趋势使得统计模型可以在优化参数上投入更多的计算力，但同时需要提高存储的利用效率，例如使用非线性处理单元。这也相应导致了机器学习和统计学的最优选择从广义线性模型及核方法变化为深度多层神经网络。这样的变化正是诸如多层感知机、卷积神经网络、长短期记忆循环神经网络和Q学习等深度学习的支柱模型在过去10年从坐了数十年的冷板凳上站起来被“重新发现”的原因。</p>
<p>系统研究者负责构建更好的工具，统计学家建立更好的模型。这样的分工使工作大大简化。举例来说，在2014年时，训练一个逻辑回归模型曾是卡内基梅隆大学布置给机器学习方向的新入学博士生的作业问题。时至今日，这个问题只需要少于10行的代码便可以完成，普通的程序员都可以做到。</p>
<h1 id="特点">特点</h1>
<p>在描述深度学习的特点之前，我们先回顾并概括一下机器学习和深度学习的关系。</p>
<p>机器学习研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。</p>
<p>在机器学习的众多研究方向中，表征学习关注如何自动找出表示数据的合适方式，以便更好地将输入变换为正确的输出，而本书要重点探讨的深度学习是具有多级表示的表征学习方法。在每一级（从原始数据开始），深度学习通过简单的函数将该级的表示变换为更高级的表示。因此，深度学习模型也可以看作是由许多简单函数复合而成的函数。当这些复合的函数足够多时，深度学习模型就可以表达非常复杂的变换。</p>
<p>深度学习可以逐级表示越来越抽象的概念或模式。以图像为例，它的输入是一堆原始像素值。深度学习模型中，图像可以逐级表示为特定位置和角度的边缘、由边缘组合得出的花纹、由多种花纹进一步汇合得到的特定部位的模式等。最终，模型能够较容易根据更高级的表示完成给定的任务，如识别图像中的物体。值得一提的是，作为表征学习的一种，深度学习将自动找出每一级表示数据的合适方式。</p>
<h1 id="总结">总结</h1>
<ul>
<li>机器学习研究如何使计算机系统利用经验改善性能。它是人工智能领域的分支，也是实现人工智能的一种手段。</li>
<li>作为机器学习的一类，表征学习关注如何自动找出表示数据的合适方式。</li>
<li>深度学习是具有多级表示的表征学习方法。它可以逐级表示越来越抽象的概念或模式。</li>
<li>深度学习所基于的神经网络模型和用数据编程的核心思想实际上已经被研究了数百年。</li>
<li>深度学习已经逐渐演变成一个工程师和科学家皆可使用的普适工具。</li>
</ul>
<h1 id="环境配置">环境配置</h1>
<p>貌似主要的问题是装一个 pytorch，jupyter 我暂时不打算装，那么单独开一个 conda 环境来装 torch 好了。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> activate</span><br><span class="line">conda create -n torch python=3.8</span><br><span class="line">&gt; conda activate torch</span><br></pre></td></tr></table></figure>
<p>直接去<a target="_blank" rel="noopener" href="https://pytorch.org/">PyTorch官网</a>找到自己的软硬件对应的安装命令即可</p>
<p>注意，conda 换为清华源，否则慢死了，<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/7e663bb0d904">换的方法很简单</a></p>
<h2 id="如何避开-jupyter"><del>如何避开 jupyter</del></h2>
<p>破防了，我不想安装 jupytor ，估计<a target="_blank" rel="noopener" href="https://vimsky.com/zh-tw/examples/detail/python-method-IPython.embed.html">用 embed 在 pycharm 里也能这么玩</a>。</p>
<p>所以在你的 conda 环境里：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; conda install IPython</span><br></pre></td></tr></table></figure>
<p>其实很简单，我比较习惯单独给 python 开环境，base 环境是不装东西的，所以我先开一个 conda 环境，像上面那样写个安装 torch，然后如下几行代码，手动实现 jupyter note book</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; touch torch_learn.py</span><br><span class="line">&gt; <span class="built_in">echo</span> <span class="string">&quot;from IPython import embed&quot;</span> &gt;&gt; torch_learn.py</span><br><span class="line">&gt; <span class="built_in">echo</span> <span class="string">&quot;embed()&quot;</span> &gt;&gt; torch_learn.py</span><br><span class="line">&gt; <span class="built_in">source</span> activate</span><br><span class="line">&gt; conda activate torch</span><br><span class="line">&gt; python3 torch_learn.py</span><br><span class="line">Python 3.8.12 (default, Oct 12 2021, 06:23:56)</span><br><span class="line">Type <span class="string">&#x27;copyright&#x27;</span>, <span class="string">&#x27;credits&#x27;</span> or <span class="string">&#x27;license&#x27;</span> <span class="keyword">for</span> more information</span><br><span class="line">IPython 7.31.1 -- An enhanced Interactive Python. Type <span class="string">&#x27;?&#x27;</span> <span class="keyword">for</span> <span class="built_in">help</span>.</span><br><span class="line"></span><br><span class="line">In [1]:</span><br></pre></td></tr></table></figure>
<h2 id="老实用-jupyter">老实用 jupyter</h2>
<p>不得不说，现在( 2022 年 4 月 2 日) 觉得，以前不用 jupyter 纯粹是自己耍小聪明。如果一直拒绝新工具，我现在还在用 word 写笔记吧，就像以前那个自以为是的自己一样吧，虽然现在总觉得仍然是自以为是…</p>
<p>参考了<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33105153">这篇笔记</a>，人家写的很清楚( 甚至认为你没有 linux 基础 )，我写点我自己遇到的问题。当然，如果没问题，就不写了…</p>
<h1 id="数据操作">数据操作</h1>
<p>在PyTorch中，<code>torch.Tensor</code>是存储和变换数据的主要工具。如果你之前用过NumPy，你会发现<code>Tensor</code>和NumPy的多维数组非常类似。然而，<code>Tensor</code>提供GPU计算和自动求梯度等更多功能，这些使<code>Tensor</code>更加适合深度学习。</p>
<h2 id="创建-tensor">创建 Tensor</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> embed</span><br><span class="line"></span><br><span class="line">x = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">embed()</span><br></pre></td></tr></table></figure>
<p>如果您现在不清楚 embed() 的含义，其实完全可以用 print(x)</p>
<p>当然，我在运行这个程序的时候，出了问题：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; python3 torch.py</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;torch.py&quot;</span>, line 1, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    import torch</span><br><span class="line">  File <span class="string">&quot;/Users/zhaochen20/CST_THU/2022_winter/科研/torch.py&quot;</span>, line 4, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    x = torch.empty(5, 3)</span><br><span class="line">AttributeError: partially initialized module <span class="string">&#x27;torch&#x27;</span> has no attribute <span class="string">&#x27;empty&#x27;</span> (most likely due to a circular import)</span><br></pre></td></tr></table></figure>
<p>貌似是自掘坟墓，因为我的执行文件名字叫做 torch，然后引入了一个包也是 torch</p>
<p>重新命名就好了</p>
<p>话说回来，torch 和 IPython 里面铁定有个库特别大，import 了特别久</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">----------------------------</span><br><span class="line">import torch</span><br><span class="line">from IPython import embed</span><br><span class="line"></span><br><span class="line">x = torch.empty(5, 3)</span><br><span class="line">embed()</span><br><span class="line">----------------------------</span><br><span class="line">&gt; python3 torch_tutorial.py</span><br><span class="line">Python 3.8.12 (default, Oct 12 2021, 06:23:56) </span><br><span class="line">Type <span class="string">&#x27;copyright&#x27;</span>, <span class="string">&#x27;credits&#x27;</span> or <span class="string">&#x27;license&#x27;</span> <span class="keyword">for</span> more information</span><br><span class="line">IPython 7.31.1 -- An enhanced Interactive Python. Type <span class="string">&#x27;?&#x27;</span> <span class="keyword">for</span> <span class="built_in">help</span>.</span><br><span class="line"></span><br><span class="line">In [1]: x</span><br><span class="line">Out[1]: </span><br><span class="line">tensor([[8.1716e-07, 8.2022e+17, 1.3556e-19],</span><br><span class="line">        [1.3563e-19, 2.6056e-12, 9.9628e+17],</span><br><span class="line">        [1.3556e-19, 1.3563e-19, 4.1294e-08],</span><br><span class="line">        [1.6496e-07, 1.1704e-19, 1.3563e-19],</span><br><span class="line">        [3.1887e-09, 2.3308e-09, 3.1458e-12]])</span><br></pre></td></tr></table></figure>
<p>完成了第一次 import 后，之后就快了很多，估计是 b 树的什么机制吧</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[0.4963, 0.7682, 0.0885],</span><br><span class="line">        [0.1320, 0.3074, 0.6341],</span><br><span class="line">        [0.4901, 0.8964, 0.4556],</span><br><span class="line">        [0.6323, 0.3489, 0.4017],</span><br><span class="line">        [0.0223, 0.1689, 0.2939]])</span><br></pre></td></tr></table></figure>
<p>还可以通过现有的<code>Tensor</code>来创建，此方法会默认重用输入<code>Tensor</code>的一些属性，例如数据类型，除非自定义数据类型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = x.new_ones(<span class="number">5</span>, <span class="number">3</span>, dtype=torch.float64)  </span><br><span class="line"><span class="comment"># 返回的tensor默认具有相同的torch.dtype和torch.device</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">x = torch.randn_like(x, dtype=torch.<span class="built_in">float</span>) </span><br><span class="line"><span class="comment"># 指定新的数据类型</span></span><br><span class="line"><span class="built_in">print</span>(x) </span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]], dtype=torch.float64)</span><br><span class="line">tensor([[ 0.6035,  0.8110, -0.0451],</span><br><span class="line">        [ 0.8797,  1.0482, -0.0445],</span><br><span class="line">        [-0.7229,  2.8663, -0.5655],</span><br><span class="line">        [ 0.1604, -0.0254,  1.0739],</span><br><span class="line">        [ 2.2628, -0.9175, -0.2251]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.size())</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">torch.Size([5, 3])</span><br><span class="line">torch.Size([5, 3])</span><br></pre></td></tr></table></figure>
<p>注意：返回的 torch.Size 其实就是一个 tuple, 支持所有tuple的操作。</p>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Tensor(*sizes)</td>
<td>基础构造函数</td>
</tr>
<tr class="even">
<td>tensor(data,)</td>
<td>类似np.array的构造函数</td>
</tr>
<tr class="odd">
<td>ones(*sizes)</td>
<td>全1Tensor</td>
</tr>
<tr class="even">
<td>zeros(*sizes)</td>
<td>全0Tensor</td>
</tr>
<tr class="odd">
<td>eye(*sizes)</td>
<td>对角线为1，其他为0</td>
</tr>
<tr class="even">
<td>arange(s,e,step)</td>
<td>从s到e，步长为step</td>
</tr>
<tr class="odd">
<td>linspace(s,e,steps)</td>
<td>从s到e，均匀切分成steps份</td>
</tr>
<tr class="even">
<td>rand/randn(*sizes)</td>
<td>均匀/标准分布</td>
</tr>
<tr class="odd">
<td>normal(mean,std)/uniform(from,to)</td>
<td>正态分布/均匀分布</td>
</tr>
<tr class="even">
<td>randperm(m)</td>
<td>随机排列</td>
</tr>
</tbody>
</table>
<h2 id="算数操作">算数操作</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line">--------------------</span><br><span class="line"><span class="built_in">print</span>(torch.add(x, y))</span><br><span class="line">--------------------</span><br><span class="line">result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=result)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line">--------------------</span><br><span class="line"><span class="comment"># adds x to y</span></span><br><span class="line">y.add_(x)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br></pre></td></tr></table></figure>
<p><strong>注：PyTorch 操作 inplace 版本都有后缀 <code>_</code>, 例如 <code>x.copy_(y), x.t_()</code></strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = x[<span class="number">0</span>, :]</span><br><span class="line">y += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>, :]) <span class="comment"># 源tensor也被改了</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>index_select(input, dim, index)</td>
<td>在指定维度dim上选取，比如选取某些行、某些列</td>
</tr>
<tr class="even">
<td>masked_select(input, mask)</td>
<td>例子如上，a[a&gt;0]，使用ByteTensor进行选取</td>
</tr>
<tr class="odd">
<td>nonzero(input)</td>
<td>非0元素的下标</td>
</tr>
<tr class="even">
<td>gather(input, dim, index)</td>
<td>根据index，在dim维度上选取数据，输出的size与index一样</td>
</tr>
</tbody>
</table>
<p>这里不详细介绍，用到了再查官方文档。</p>
<h2 id="改变形状">改变形状</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">3</span>]: x = torch.rand(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">In [<span class="number">4</span>]: x.size()</span><br><span class="line">Out[<span class="number">4</span>]: torch.Size([<span class="number">5</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<p>这就解释了 <code>y = x.view(15)</code> 和 <code>y = x.view(x.size())</code> 效果不同</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">5</span>]: y = x.view(<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: y</span><br><span class="line">Out[<span class="number">6</span>]:</span><br><span class="line">tensor([<span class="number">0.6734</span>, <span class="number">0.6783</span>, <span class="number">0.2743</span>, <span class="number">0.2130</span>, <span class="number">0.0791</span>, <span class="number">0.6645</span>, <span class="number">0.1746</span>, <span class="number">0.4423</span>, <span class="number">0.0358</span>,</span><br><span class="line">        <span class="number">0.9683</span>, <span class="number">0.1560</span>, <span class="number">0.1490</span>, <span class="number">0.7802</span>, <span class="number">0.5694</span>, <span class="number">0.6750</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">7</span>]: z = x.view(-<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: z</span><br><span class="line">Out[<span class="number">8</span>]:</span><br><span class="line">tensor([[<span class="number">0.6734</span>, <span class="number">0.6783</span>, <span class="number">0.2743</span>, <span class="number">0.2130</span>, <span class="number">0.0791</span>],</span><br><span class="line">        [<span class="number">0.6645</span>, <span class="number">0.1746</span>, <span class="number">0.4423</span>, <span class="number">0.0358</span>, <span class="number">0.9683</span>],</span><br><span class="line">        [<span class="number">0.1560</span>, <span class="number">0.1490</span>, <span class="number">0.7802</span>, <span class="number">0.5694</span>, <span class="number">0.6750</span>]])</span><br></pre></td></tr></table></figure>
<p><strong>注意 <code>view()</code> 返回的新 <code>Tensor</code> 与源 <code>Tensor</code> 虽然可能有不同的 <code>size</code>，但是是共享 <code>data</code> 的，也即更改其中的一个，另外一个也会跟着改变。(顾名思义，view 仅仅是改变了对这个张量的观察角度，内部数据并未改变)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">9</span>]: x += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: x</span><br><span class="line">Out[<span class="number">10</span>]:</span><br><span class="line">tensor([[<span class="number">1.6734</span>, <span class="number">1.6783</span>, <span class="number">1.2743</span>],</span><br><span class="line">        [<span class="number">1.2130</span>, <span class="number">1.0791</span>, <span class="number">1.6645</span>],</span><br><span class="line">        [<span class="number">1.1746</span>, <span class="number">1.4423</span>, <span class="number">1.0358</span>],</span><br><span class="line">        [<span class="number">1.9683</span>, <span class="number">1.1560</span>, <span class="number">1.1490</span>],</span><br><span class="line">        [<span class="number">1.7802</span>, <span class="number">1.5694</span>, <span class="number">1.6750</span>]])</span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: y</span><br><span class="line">Out[<span class="number">11</span>]:</span><br><span class="line">tensor([<span class="number">1.6734</span>, <span class="number">1.6783</span>, <span class="number">1.2743</span>, <span class="number">1.2130</span>, <span class="number">1.0791</span>, <span class="number">1.6645</span>, <span class="number">1.1746</span>, <span class="number">1.4423</span>, <span class="number">1.0358</span>,</span><br><span class="line">        <span class="number">1.9683</span>, <span class="number">1.1560</span>, <span class="number">1.1490</span>, <span class="number">1.7802</span>, <span class="number">1.5694</span>, <span class="number">1.6750</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: z</span><br><span class="line">Out[<span class="number">12</span>]:</span><br><span class="line">tensor([[<span class="number">1.6734</span>, <span class="number">1.6783</span>, <span class="number">1.2743</span>, <span class="number">1.2130</span>, <span class="number">1.0791</span>],</span><br><span class="line">        [<span class="number">1.6645</span>, <span class="number">1.1746</span>, <span class="number">1.4423</span>, <span class="number">1.0358</span>, <span class="number">1.9683</span>],</span><br><span class="line">        [<span class="number">1.1560</span>, <span class="number">1.1490</span>, <span class="number">1.7802</span>, <span class="number">1.5694</span>, <span class="number">1.6750</span>]])</span><br></pre></td></tr></table></figure>
<p>所以如果我们想返回一个真正新的副本（即不共享data内存）该怎么办呢？Pytorch还提供了一个<code>reshape()</code>可以改变形状，但是此函数并不能保证返回的是其拷贝，所以不推荐使用。推荐先用<code>clone</code>创造一个副本然后再使用<code>view</code>，<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch">参考此处</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_cp = x.clone().view(<span class="number">15</span>)</span><br><span class="line">x -= <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x_cp)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.6035,  0.8110, -0.0451],</span><br><span class="line">        [ 0.8797,  1.0482, -0.0445],</span><br><span class="line">        [-0.7229,  2.8663, -0.5655],</span><br><span class="line">        [ 0.1604, -0.0254,  1.0739],</span><br><span class="line">        [ 2.2628, -0.9175, -0.2251]])</span><br><span class="line">tensor([1.6035, 1.8110, 0.9549, 1.8797, 2.0482, 0.9555, 0.2771, 3.8663, 0.4345,</span><br><span class="line">        1.1604, 0.9746, 2.0739, 3.2628, 0.0825, 0.7749])</span><br></pre></td></tr></table></figure>
<p>使用 <code>clone</code> 还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源 <code>Tensor</code>。</p>
<p>另外一个常用的函数就是<code>item()</code>, 它可以将一个标量<code>Tensor</code>转换成一个Python number：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.item())</span><br><span class="line">---------------</span><br><span class="line">tensor([<span class="number">2.3466</span>])</span><br><span class="line"><span class="number">2.3466382026672363</span></span><br></pre></td></tr></table></figure>
<table>
<thead>
<tr class="header">
<th>函数</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>trace</td>
<td>对角线元素之和(矩阵的迹)</td>
</tr>
<tr class="even">
<td>diag</td>
<td>对角线元素</td>
</tr>
<tr class="odd">
<td>triu/tril</td>
<td>矩阵的上三角/下三角，可指定偏移量</td>
</tr>
<tr class="even">
<td>mm/bmm</td>
<td>矩阵乘法，batch的矩阵乘法</td>
</tr>
<tr class="odd">
<td>addmm/addbmm/addmv/addr/baddbmm..</td>
<td>矩阵运算</td>
</tr>
<tr class="even">
<td>t</td>
<td>转置</td>
</tr>
<tr class="odd">
<td>dot/cross</td>
<td>内积/外积</td>
</tr>
<tr class="even">
<td>inverse</td>
<td>求逆矩阵</td>
</tr>
<tr class="odd">
<td>svd</td>
<td>奇异值分解</td>
</tr>
</tbody>
</table>
<p>PyTorch中的 <code>Tensor</code> 支持超过一百种操作，包括转置、索引、切片、数学运算、线性代数、随机数等等，可参考<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/tensors.html">官方文档</a>。</p>
<h2 id="广播">广播</h2>
<p>前面我们看到如何对两个形状相同的 <code>Tensor</code> 做按元素运算。当对两个形状不同的 <code>Tensor</code> 按元素运算时，可能会触发广播（broadcasting）机制：先适当复制元素使这两个 <code>Tensor</code> 形状相同后再按元素运算。例如：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">3</span>).view(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">y = torch.arange(<span class="number">1</span>, <span class="number">4</span>).view(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(x + y)</span><br><span class="line">-------------------</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>]])</span><br><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>]])</span><br></pre></td></tr></table></figure>
<p>由于 <code>x</code> 和 <code>y</code> 分别是 1 行 2 列和 3 行 1 列的矩阵，如果要计算 <code>x + y</code>，那么 <code>x</code> 中第一行的 2 个元素被广播（复制）到了第二行和第三行，而 <code>y</code> 中第一列的 3 个元素被广播（复制）到了第二列。如此，就可以对 2 个 3 行 2 列的矩阵按元素相加。</p>
<p>前面说了，索引操作是不会开辟新内存的，而像<code>y = x + y</code>这样的运算是会新开内存的，然后将<code>y</code>指向新内存。为了演示这一点，我们可以使用Python自带的<code>id</code>函数：如果两个实例的ID一致，那么它们所对应的内存地址相同；反之则不同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">y = y + x</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: id_before</span><br><span class="line">Out[<span class="number">14</span>]: <span class="number">140542217495392</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: <span class="built_in">id</span>(y)</span><br><span class="line">Out[<span class="number">15</span>]: <span class="number">140541936727840</span></span><br></pre></td></tr></table></figure>
<p>如果想指定结果到原来的<code>y</code>的内存，我们可以使用前面介绍的索引来进行替换操作。在下面的例子中，我们把<code>x + y</code>的结果通过<code>[:]</code>写进<code>y</code>对应的内存中。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">y[:] = y + x</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: id_before</span><br><span class="line">Out[<span class="number">18</span>]: <span class="number">140542217449200</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">19</span>]: <span class="built_in">id</span>(y)</span><br><span class="line">Out[<span class="number">19</span>]: <span class="number">140542217449200</span></span><br></pre></td></tr></table></figure>
<p>我们还可以使用运算符全名函数中的<code>out</code>参数或者自加运算符<code>+=</code>(也即<code>add_()</code>)达到上述效果，例如<code>torch.add(x, y, out=y)</code>和<code>y += x</code>(<code>y.add_(x)</code>)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">y = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">id_before = <span class="built_in">id</span>(y)</span><br><span class="line">torch.add(x, y, out=y) <span class="comment"># y += x, y.add_(x)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">id</span>(y) == id_before) <span class="comment"># True</span></span><br></pre></td></tr></table></figure>
<p>注：虽然<code>view</code>返回的<code>Tensor</code>与源<code>Tensor</code>是共享<code>data</code>的，但是依然是一个新的<code>Tensor</code>（因为<code>Tensor</code>除了包含<code>data</code>外还有一些其他属性），二者id（内存地址）并不一致。</p>
<p>我们很容易用<code>numpy()</code>和<code>from_numpy()</code>将<code>Tensor</code>和NumPy中的数组相互转换。但是需要注意的一点是：</p>
<p><strong>这两个函数所产生的的<code>Tensor</code>和NumPy中的数组共享相同的内存（所以他们之间的转换很快），改变其中一个时另一个也会改变！！！</strong></p>
<p>还有一个常用的将 NumPy 中的 array 转换成 <code>Tensor</code> 的方法就是 <code>torch.tensor()</code>， 需要注意的是，此方法总是会进行数据拷贝（就会消耗更多的时间和空间），所以返回的 <code>Tensor</code> 和原来的数据不再共享内存。</p>
<h2 id="gpu">GPU</h2>
<p>用方法 <code>to()</code> 可以将 <code>Tensor</code> 在 CPU 和 GPU（需要硬件支持）之间相互移动。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">20</span>]: torch.cuda.is_available()</span><br><span class="line">Out[<span class="number">20</span>]: <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<p><del>破防了</del></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以下代码只有在PyTorch GPU版本上才会执行</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span>)          <span class="comment"># GPU</span></span><br><span class="line">    y = torch.ones_like(x, device=device)  <span class="comment"># 直接创建一个在GPU上的Tensor</span></span><br><span class="line">    x = x.to(device)                       <span class="comment"># 等价于 .to(&quot;cuda&quot;)</span></span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="built_in">print</span>(z)</span><br><span class="line">    <span class="built_in">print</span>(z.to(<span class="string">&quot;cpu&quot;</span>, torch.double))       <span class="comment"># to()还可以同时更改数据类型</span></span><br></pre></td></tr></table></figure>
<h2 id="自动求解梯度">自动求解梯度</h2>
<p>在深度学习中，我们经常需要对函数求梯度（gradient）。PyTorch 提供的 <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/autograd.html">autograd</a> 包能够根据输入和前向传播过程自动构建计算图，并执行反向传播。本节将介绍如何使用 autograd 包来进行自动求梯度的有关操作。</p>
<p>上一节介绍的 <code>Tensor</code> 是这个包的核心类，如果将其属性 <code>.requires_grad</code> 设置为 <code>True</code>，它将开始追踪( track )在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用 <code>.backward()</code> 来完成所有梯度计算。此 <code>Tensor</code> 的梯度将累积到 <code>.grad</code> 属性中。</p>
<blockquote>
<p>注意在<code>y.backward()</code>时，如果<code>y</code>是标量，则不需要为<code>backward()</code>传入任何参数；否则，需要传入一个与<code>y</code>同形的 <code>Tensor</code> 作为权重。解释见 2.3.2 节。</p>
</blockquote>
<p>如果不想要被继续追踪，可以调用<code>.detach()</code>将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用<code>with torch.no_grad()</code>将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数（<code>requires_grad=True</code>）的梯度。</p>
<p><code>Function</code>是另外一个很重要的类。<code>Tensor</code>和<code>Function</code>互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个<code>Tensor</code>都有一个<code>.grad_fn</code>属性，该属性即创建该<code>Tensor</code>的<code>Function</code>, 就是说该<code>Tensor</code>是不是通过某些运算得到的，若是，则<code>grad_fn</code>返回一个与这些运算相关的对象，否则是None。</p>
<h3 id="tensor">Tensor</h3>
<p>创建一个<code>Tensor</code>并设置<code>requires_grad=True</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x.grad_fn)</span><br><span class="line">---------------</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="literal">None</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y = x + <span class="number">2</span></span><br><span class="line"><span class="built_in">print</span>(y)</span><br><span class="line"><span class="built_in">print</span>(y.grad_fn)</span><br><span class="line">---------------</span><br><span class="line">tensor([[<span class="number">3.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>]], grad_fn=&lt;AddBackward&gt;)</span><br><span class="line">&lt;AddBackward <span class="built_in">object</span> at <span class="number">0x1100477b8</span>&gt;</span><br></pre></td></tr></table></figure>
<p>注意x是直接创建的，所以它没有<code>grad_fn</code>, 而y是通过一个加法操作创建的，所以它有一个为<code>&lt;AddBackward&gt;</code>的<code>grad_fn</code>。</p>
<p>像x这种直接创建的称为叶子节点，叶子节点对应的<code>grad_fn</code>是<code>None</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.is_leaf, y.is_leaf) </span><br><span class="line"><span class="comment"># True False</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">z = y * y * <span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line"><span class="built_in">print</span>(z, out)</span><br><span class="line">--------------</span><br><span class="line">tensor([[<span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>]], grad_fn=&lt;MulBackward&gt;) tensor(<span class="number">27.</span>, grad_fn=&lt;MeanBackward1&gt;)</span><br></pre></td></tr></table></figure>
<p>通过<code>.requires_grad_()</code>来用in-place的方式改变<code>requires_grad</code>属性：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>) </span><br><span class="line"><span class="comment"># 缺失情况下默认 requires_grad = False</span></span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad) <span class="comment"># False</span></span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(a.requires_grad) <span class="comment"># True</span></span><br><span class="line">b = (a * a).<span class="built_in">sum</span>()</span><br><span class="line"><span class="built_in">print</span>(b.grad_fn)</span><br><span class="line">--------------</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="literal">True</span></span><br><span class="line">&lt;SumBackward0 <span class="built_in">object</span> at <span class="number">0x118f50cc0</span>&gt;</span><br></pre></td></tr></table></figure>
<h3 id="梯度">梯度</h3>
<p>因为<code>out</code>是一个标量，所以调用<code>backward()</code>时不需要指定求导变量：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">out.backward() </span><br><span class="line"><span class="comment"># 等价于 out.backward(torch.tensor(1.))</span></span><br></pre></td></tr></table></figure>
<p>我们来看看<code>out</code>关于<code>x</code>的梯度 <span class="math inline">\(\frac{d(out)}{dx}\)</span>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">----------------</span><br><span class="line">tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">23</span>]: x = torch.ones(<span class="number">2</span>, <span class="number">2</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">24</span>]: y = x + <span class="number">2</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">25</span>]: z = y * y * <span class="number">3</span></span><br><span class="line">    ...: out = z.mean()</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: z</span><br><span class="line">Out[<span class="number">26</span>]:</span><br><span class="line">tensor([[<span class="number">27.</span>, <span class="number">27.</span>],</span><br><span class="line">        [<span class="number">27.</span>, <span class="number">27.</span>]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: out</span><br><span class="line">Out[<span class="number">27</span>]: tensor(<span class="number">27.</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br><span class="line">  </span><br><span class="line">In [<span class="number">31</span>]: out.backward() <span class="comment"># 等价于 out.backward(torch.tensor(1.))</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">32</span>]: <span class="built_in">print</span>(x.grad)</span><br><span class="line">tensor([[<span class="number">4.5000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">4.5000</span>, <span class="number">4.5000</span>]])</span><br></pre></td></tr></table></figure>
<p>注意，如果不手动添加 <code>out.backward()</code>，你的 <code>out</code> 没法计算梯度</p>
<p>我们令<code>out</code>为 <span class="math inline">\(f\)</span> , 因为 <span class="math display">\[
f=\frac14\sum_{i=1}^4z_i=\frac14\sum_{i=1}^43(x_i+2)^2\\
 \frac{\partial{o}}{\partial{x_i}}\bigr\rvert_{x_i=1}=\frac{9}{2}=4.5
\]</span></p>
<p>数学上，如果有一个函数值和自变量都为向量的函数 <span class="math inline">\(\vec{y}=f(\vec{x})\)</span>, 那么 <span class="math inline">\(\vec{y}\)</span> 关于 <span class="math inline">\(\vec{x}\)</span> 的梯度就是一个雅可比矩阵（Jacobian matrix）: <span class="math display">\[
J=\left(\begin{array}{ccc} \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\\vdots &amp; \ddots &amp; \vdots\\\frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}} \end{array}\right) 
\]</span></p>
<p>而 <code>torch.autograd</code> 这个包就是用来计算一些雅克比矩阵的乘积的。例如，如果 <span class="math inline">\(v\)</span> 是一个标量函数的 <span class="math inline">\(l=g\left(\vec{y}\right)\)</span> 的梯度： <span class="math display">\[ v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right) \]</span> 那么根据链式法则我们有 <span class="math inline">\(l\)</span> 关于 <span class="math inline">\(\vec{x}\)</span> 的雅克比矩阵就为: <span class="math display">\[
J_v=\left(\begin{array}{ccc}\frac{\partial l}{\partial y_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial y_{m}}\end{array}\right) \left(\begin{array}{ccc} \frac{\partial y_{1}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{1}}{\partial x_{n}}\\\vdots &amp; \ddots &amp; \vdots\\\frac{\partial y_{m}}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial y_{m}}{\partial x_{n}} \end{array}\right) =\left(\begin{array}{ccc}\frac{\partial l}{\partial x_{1}} &amp; \cdots &amp; \frac{\partial l}{\partial x_{n}}\end{array}\right)
\]</span> 注意：grad在反向传播过程中是累加的(accumulated)，这意味着每一次运行反向传播，梯度都会累加之前的梯度，所以一般在反向传播之前需把梯度清零。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 再来反向传播一次，注意grad是累加的</span></span><br><span class="line">out2 = x.<span class="built_in">sum</span>()</span><br><span class="line">out2.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line"></span><br><span class="line">out3 = x.<span class="built_in">sum</span>()</span><br><span class="line">x.grad.data.zero_()</span><br><span class="line">out3.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">---------------</span><br><span class="line">tensor([[<span class="number">5.5000</span>, <span class="number">5.5000</span>],</span><br><span class="line">        [<span class="number">5.5000</span>, <span class="number">5.5000</span>]])</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
<p>现在我们解释2.3.1节留下的问题，为什么在<code>y.backward()</code>时，如果<code>y</code>是标量，则不需要为<code>backward()</code>传入任何参数；否则，需要传入一个与<code>y</code>同形的<code>Tensor</code>? 简单来说就是为了避免向量（甚至更高维张量）对张量求导，而转换成标量对张量求导。举个例子，假设形状为 <code>m x n</code> 的矩阵 X 经过运算得到了 <code>p x q</code> 的矩阵 Y，Y 又经过运算得到了 <code>s x t</code> 的矩阵 Z。那么按照前面讲的规则，dZ/dY 应该是一个 <code>s x t x p x q</code> 四维张量，dY/dX 是一个 <code>p x q x m x n</code>的四维张量。问题来了，怎样反向传播？怎样将两个四维张量相乘？？？这要怎么乘？？？就算能解决两个四维张量怎么乘的问题，四维和三维的张量又怎么乘？导数的导数又怎么求，这一连串的问题，感觉要疯掉…… 为了避免这个问题，我们<strong>不允许张量对张量求导，只允许标量对张量求导，求导结果是和自变量同形的张量</strong>。所以必要时我们要把张量通过将所有张量的元素加权求和的方式转换为标量，举个例子，假设<code>y</code>由自变量<code>x</code>计算而来，<code>w</code>是和<code>y</code>同形的张量，则<code>y.backward(w)</code>的含义是：先计算<code>l = torch.sum(y * w)</code>，则<code>l</code>是个标量，然后求<code>l</code>对自变量<code>x</code>的导数——<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29923090">参考</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">z = y.view(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(z)</span><br><span class="line">-----------------</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">4.</span>],</span><br><span class="line">        [<span class="number">6.</span>, <span class="number">8.</span>]], grad_fn=&lt;ViewBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>现在 <code>z</code> 不是一个标量，所以在调用<code>backward</code>时需要传入一个和<code>z</code>同形的权重向量进行加权求和得到一个标量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">v = torch.tensor([[<span class="number">1.0</span>, <span class="number">0.1</span>], [<span class="number">0.01</span>, <span class="number">0.001</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">z.backward(v)</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">--------------------</span><br><span class="line">tensor([<span class="number">2.0000</span>, <span class="number">0.2000</span>, <span class="number">0.0200</span>, <span class="number">0.0020</span>])</span><br></pre></td></tr></table></figure>
<p>注意到，<code>v</code> 是权重，然后 <code>z</code> 张量转为了标量后再对 <code>x</code> 张量求导，<code>x.grad</code>是和<code>x</code>同形的张量。</p>
<p>再来看看中断梯度追踪的例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y1 = x ** <span class="number">2</span> </span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    y2 = x ** <span class="number">3</span></span><br><span class="line">y3 = y1 + y2</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.requires_grad)</span><br><span class="line"><span class="built_in">print</span>(y1, y1.requires_grad) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(y2, y2.requires_grad) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(y3, y3.requires_grad) <span class="comment"># True</span></span><br><span class="line">--------------</span><br><span class="line"><span class="literal">True</span></span><br><span class="line">tensor(<span class="number">1.</span>, grad_fn=&lt;PowBackward0&gt;) <span class="literal">True</span></span><br><span class="line">tensor(<span class="number">1.</span>) <span class="literal">False</span></span><br><span class="line">tensor(<span class="number">2.</span>, grad_fn=&lt;ThAddBackward&gt;) <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<p>可以看到，上面的<code>y2</code>是没有<code>grad_fn</code>而且<code>y2.requires_grad=False</code>的，而<code>y3</code>是有<code>grad_fn</code>的。如果我们将<code>y3</code>对<code>x</code>求梯度的话会是多少呢？</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y3.backward()</span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">---------</span><br><span class="line">tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure>
<p>为什么是2呢？$ y_3 = y_1 + y_2 = x^2 + x^3$，当 <span class="math inline">\(x=1\)</span> 时 <span class="math inline">\(\frac {dy_3} {dx}\)</span> 不应该是5吗？事实上，由于 <span class="math inline">\(y_2\)</span> 的定义是被<code>torch.no_grad():</code>包裹的，所以与 <span class="math inline">\(y_2\)</span> 有关的梯度是不会回传的，只有与 <span class="math inline">\(y_1\)</span> 有关的梯度才会回传，即 <span class="math inline">\(x^2\)</span> 对 <span class="math inline">\(x\)</span> 的梯度。</p>
<p>上面提到，<code>y2.requires_grad=False</code>，所以不能调用 <code>y2.backward()</code>，会报错：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn</span><br></pre></td></tr></table></figure>
<p>此外，如果我们想要修改<code>tensor</code>的数值，但是又不希望被<code>autograd</code>记录（即不会影响反向传播），那么我么可以对<code>tensor.data</code>进行操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = torch.ones(<span class="number">1</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.data) <span class="comment"># 还是一个tensor</span></span><br><span class="line"><span class="built_in">print</span>(x.data.requires_grad) <span class="comment"># 但是已经是独立于计算图之外</span></span><br><span class="line"></span><br><span class="line">y = <span class="number">2</span> * x</span><br><span class="line">x.data *= <span class="number">100</span> <span class="comment"># 只改变了值，不会记录在计算图，所以不会影响梯度传播</span></span><br><span class="line"></span><br><span class="line">y.backward()</span><br><span class="line"><span class="built_in">print</span>(x) <span class="comment"># 更改data的值也会影响tensor的值</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)</span><br><span class="line">--------------------</span><br><span class="line">tensor([<span class="number">1.</span>])</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">tensor([<span class="number">100.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">2.</span>])</span><br></pre></td></tr></table></figure>
<h1 id="深度学习基础">深度学习基础</h1>
<h2 id="线性回归">线性回归</h2>
<p>线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。</p>
<p>softmax回归适用于分类问题。</p>
<p>由于线性回归和softmax回归都是单层神经网络，它们涉及的概念和技术同样适用于大多数的深度学习模型。我们首先以线性回归为例，介绍大多数深度学习模型的基本要素和表示方法。</p>
<h3 id="线性回归的基本要素">线性回归的基本要素</h3>
<p>我们以一个简单的房屋价格预测作为例子来解释线性回归的基本要素。这个应用的目标是预测一栋房子的售出价格（元）。我们知道这个价格取决于很多因素，如房屋状况、地段、市场行情等。为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。</p>
<h3 id="定义模型">定义模型</h3>
<p>设房屋的面积为 <span class="math inline">\(x_1\)</span>，房龄为 <span class="math inline">\(x_2\)</span>，售出价格为 <span class="math inline">\(y\)</span>。我们需要建立基于输入 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span> 来计算输出 <span class="math inline">\(y\)</span> 的表达式，也就是模型（model）。顾名思义，线性回归假设输出与各个输入之间是线性关系： <span class="math display">\[
\hat{y} = x_1 w_1 + x_2 w_2 + b
\]</span> 其中 <span class="math inline">\(w_1\)</span> 和 <span class="math inline">\(w_2\)</span> 是权重（weight），<span class="math inline">\(b\)</span> 是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。模型输出 <span class="math inline">\(\hat{y}\)</span> 是线性回归对真实价格 <span class="math inline">\(y\)</span> 的预测或估计。我们通常允许它们之间有一定误差。</p>
<h3 id="模型训练">模型训练</h3>
<p>接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）。下面我们介绍模型训练所涉及的3个要素。</p>
<h4 id="训练数据">(1) 训练数据</h4>
<p>我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。</p>
<p>假设我们采集的样本数为 <span class="math inline">\(n\)</span>，索引为 <span class="math inline">\(i\)</span> 的样本的特征为 <span class="math inline">\(x_1^{(i)}\)</span> 和 <span class="math inline">\(x_2^{(i)}\)</span>，标签为 <span class="math inline">\(y^{(i)}\)</span>。对于索引为 <span class="math inline">\(i\)</span> 的房屋，线性回归模型的房屋价格预测表达式为 <span class="math display">\[ \hat{y}^{(i)} = x_1^{(i)} w_1 + x_2^{(i)} w_2 + b \]</span></p>
<h4 id="损失函数">(2) 损失函数</h4>
<p>在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个<strong>非负数</strong>作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。它在评估索引为 <span class="math inline">\(i\)</span> 的样本误差的表达式为 <span class="math display">\[
\ell^{(i)}(w_1, w_2, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2
\]</span> <strong>其中常数 <span class="math inline">\(\frac 1 2\)</span> 使对平方项求导后的常数系数为1，这样在形式上稍微简单一些。</strong></p>
<p>显然，误差越小表示预测价格与真实价格越相近，且当二者相等时误差为0。给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习里，将衡量误差的函数称为损失函数（loss function）。这里使用的平方误差函数也称为平方损失（square loss）。</p>
<p>通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量，即 <span class="math display">\[
 \ell(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \ell^{(i)}(w_1, w_2, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2
\]</span></p>
<p>在模型训练中，我们希望找出一组模型参数，记为 <span class="math inline">\(w_1^*, w_2^*, b^*\)</span>，来使训练样本平均损失最小： <span class="math display">\[
w_1^*, w_2^*, b^* = \underset{w_1, w_2, b}{\arg\min} \ell(w_1, w_2, b)
\]</span></p>
<h4 id="优化算法">(3) 优化算法</h4>
<p>当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。</p>
<p>然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。</p>
<p>在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。</p>
<p>它的算法很简单：</p>
<ul>
<li>先选取一组模型参数的初始值，如随机选取；</li>
<li>接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。</li>
<li>在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）<span class="math inline">\(\mathcal{B}\)</span>，然后求小批量中数据样本的<strong>平均损失有关模型参数的导数（梯度）</strong>，最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。</li>
</ul>
<p>在训练本节讨论的线性回归模型的过程中，模型的每个参数将作如下迭代： <span class="math display">\[
\begin{aligned} w_1 &amp;\leftarrow w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial w_1} = w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)\\\ w_2 &amp;\leftarrow w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial w_2} = w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)\\\ b &amp;\leftarrow b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial b} = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right) \end{aligned}
\]</span></p>
<p>在上式中，<span class="math inline">\(|\mathcal{B}|\)</span> 代表每个小批量中的样本个数（批量大小，batch size），<span class="math inline">\(\eta\)</span> 称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。</p>
<p><strong>我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。在少数情况下，超参数也可以通过模型训练学出。本书对此类情况不做讨论。</strong></p>
<h3 id="模型预测">模型预测</h3>
<p>模型训练完成后，我们将模型参数 <span class="math inline">\(w_1, w_2, b\)</span> 在优化算法停止时的值分别记作 <span class="math inline">\(\hat{w}_1, \hat{w}_2, \hat{b}\)</span>。注意，这里我们得到的并不一定是最小化损失函数的最优解 <span class="math inline">\(w_1^*, w_2^*, b^*\)</span>，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型 <span class="math inline">\(x_1 \hat{w}_1 + x_2 \hat{w}_2 + \hat{b}\)</span> 来估算训练数据集以外任意一栋面积（平方米）为<span class="math inline">\(x_1\)</span>、房龄（年）为<span class="math inline">\(x_2\)</span>的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。</p>
<h2 id="表示方法">表示方法</h2>
<p>此段将解释线性回归与神经网络的联系，以及线性回归的矢量计算表达式。</p>
<p>在深度学习中，我们可以使用神经网络图直观地表现模型结构。为了更清晰地展示线性回归作为神经网络的结构，图3.1使用神经网络图表示本节中介绍的线性回归模型。神经网络图隐去了模型参数权重和偏差。</p>
<p><img src="https://github.com/ShusenTang/Dive-into-DL-PyTorch/raw/master/docs/img/chapter03/3.1_linreg.svg" /></p>
<p>在图3.1所示的神经网络中，输入分别为 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span>，因此输入层的输入个数为2。输入个数也叫特征数或特征向量维度。图3.1中网络的输出为 <span class="math inline">\(o\)</span>，输出层的输出个数为1。需要注意的是，我们直接将图3.1中神经网络的输出 <span class="math inline">\(o\)</span> 作为线性回归的输出，即 <span class="math inline">\(\hat{y} = o\)</span>。由于输入层并不涉及计算，按照惯例，图3.1所示的神经网络的层数为1。所以线性回归是一个单层神经网络。输出层中负责计算 <span class="math inline">\(o\)</span> 的单元又叫神经元。在线性回归中，<span class="math inline">\(o\)</span> 的计算依赖于 <span class="math inline">\(x_1\)</span> 和 <span class="math inline">\(x_2\)</span>。也就是说，<strong>输出层中的神经元和输入层中各个输入完全连接，此种输出层又叫全连接层（fully-connected layer）或稠密层（dense layer）。</strong></p>
<h3 id="矢量计算">矢量计算</h3>
<p>下面先定义两个1000维的向量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">a = torch.ones(<span class="number">1000</span>)</span><br><span class="line">b = torch.ones(<span class="number">1000</span>)</span><br><span class="line">-----------------------</span><br><span class="line"><span class="comment"># 标量运算</span></span><br><span class="line">start = time()</span><br><span class="line">c = torch.zeros(<span class="number">1000</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    c[i] = a[i] + b[i]</span><br><span class="line"><span class="built_in">print</span>(time() - start)</span><br><span class="line">-----------------------</span><br><span class="line"><span class="number">0.02039504051208496</span></span><br><span class="line">------------------------</span><br><span class="line"><span class="comment"># 矢量运算</span></span><br><span class="line">start = time()</span><br><span class="line">d = a + b</span><br><span class="line"><span class="built_in">print</span>(time() - start)</span><br><span class="line">------------------------</span><br><span class="line"><span class="number">0.0008330345153808594</span></span><br><span class="line">------------------------</span><br></pre></td></tr></table></figure>
<p>结果很明显，后者比前者更省时。因此，我们应该尽可能采用矢量计算，以提升计算效率。</p>
<p>让我们再次回到本节的房价预测问题。如果我们对训练数据集里的3个房屋样本（索引分别为1、2和3）逐一预测价格，将得到 <span class="math display">\[
\begin{aligned} \hat{y}^{(1)} &amp;= x_1^{(1)} w_1 + x_2^{(1)} w_2 + b,\\\hat{y}^{(2)} &amp;= x_1^{(2)} w_1 + x_2^{(2)} w_2 + b,\\\hat{y}^{(3)} &amp;= x_1^{(3)} w_1 + x_2^{(3)} w_2 + b. \end{aligned}
\]</span></p>
<p>写作矢量形式： <span class="math display">\[
\boldsymbol{\hat{y}} = \begin{bmatrix} \hat{y}^{(1)} \\\hat{y}^{(2)} \\\hat{y}^{(3)} \end{bmatrix},\quad \boldsymbol{X} = \begin{bmatrix} x_1^{(1)} &amp; x_2^{(1)}\\ x_1^{(2)} &amp; x_2^{(2)} \\ x_1^{(3)} &amp; x_2^{(3)} \end{bmatrix},\quad \boldsymbol{w} = \begin{bmatrix} w_1 \\ w_2 \end{bmatrix}
\]</span> 对3个房屋样本预测价格的矢量计算表达式为<span class="math inline">\(\boldsymbol{\hat{y}} = \boldsymbol{X} \boldsymbol{w} + b,\)</span> 其中的加法运算使用了广播机制。</p>
<p>广义上讲，当数据样本数为 <span class="math inline">\(n\)</span>，特征数为 <span class="math inline">\(d\)</span> 时，线性回归的矢量计算表达式为 <span class="math display">\[ \boldsymbol{\hat{y}} = \boldsymbol{X} \boldsymbol{w} + b \]</span> 其中模型输出 <span class="math inline">\(\boldsymbol{\hat{y}} \in \mathbb{R}^{n \times 1}\)</span> 批量数据样本特征 <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{n \times d}\)</span>，权重 <span class="math inline">\(\boldsymbol{w} \in \mathbb{R}^{d \times 1}\)</span>， 偏差 <span class="math inline">\(b \in \mathbb{R}\)</span>。相应地，批量数据样本标签 <span class="math inline">\(\boldsymbol{y} \in \mathbb{R}^{n \times 1}\)</span>。设模型参数 <span class="math inline">\(\boldsymbol{\theta} = [w_1, w_2, b]^\top\)</span>，我们可以重写损失函数为 <span class="math display">\[ \ell(\boldsymbol{\theta})=\frac{1}{2n}(\boldsymbol{\hat{y}}-\boldsymbol{y})^\top(\boldsymbol{\hat{y}}-\boldsymbol{y}) \]</span></p>
<p>小批量随机梯度下降的迭代步骤将相应地改写为 <span class="math display">\[ \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \nabla_{\boldsymbol{\theta}} \ell^{(i)}(\boldsymbol{\theta}), \]</span></p>
<p>其中梯度是损失有关3个为标量的模型参数的偏导数组成的向量： <span class="math display">\[ \nabla_{\boldsymbol{\theta}} \ell^{(i)}(\boldsymbol{\theta})= \begin{bmatrix} \frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial w_1} \\\frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial w_2} \\\frac{ \partial \ell^{(i)}(w_1, w_2, b) }{\partial b} \end{bmatrix} = \begin{bmatrix} x_1^{(i)} (x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}) \\ x_2^{(i)} (x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}) \\ x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)} \end{bmatrix}= \begin{bmatrix} x_1^{(i)} \\ x_2^{(i)} \\ 1 \end{bmatrix} (\hat{y}^{(i)} - y^{(i)}) \]</span></p>
<h3 id="总结-1">总结</h3>
<ul>
<li><p>和大多数深度学习模型一样，对于线性回归这样一种单层神经网络，它的基本要素包括模型、训练数据、损失函数和优化算法。</p></li>
<li><p>既可以用神经网络图表示线性回归，又可以用矢量计算表示该模型。</p></li>
<li><p>应该尽可能采用矢量计算，以提升计算效率。</p></li>
</ul>
<h1 id="代码实现">代码实现</h1>
<p>在了解了线性回归的背景知识之后，现在我们可以动手实现它了。尽管强大的深度学习框架可以减少大量重复性工作，但若过于依赖它提供的便利，会导致我们很难深入理解深度学习是如何工作的。因此，本节将介绍如何只利用 <code>Tensor</code> 和 <code>autograd</code> 来实现一个线性回归的训练。</p>
<p>首先，导入本节中实验所需的包或模块，其中的matplotlib包可用于作图，且设置成嵌入显示。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure>
<p>该下载的包自己 conda 安装就好了。</p>
<h2 id="生成数据集">生成数据集</h2>
<p>我们构造一个简单的人工训练数据集，它可以使我们能够直观比较学到的参数和真实的模型参数的区别。设训练数据集样本数为1000，输入特征数为2。给定随机生成的批量样本特征 <span class="math inline">\(\boldsymbol{X} \in \mathbb{R}^{1000 \times 2}\)</span>，我们使用线性回归模型真实权重 <span class="math inline">\(\boldsymbol{w} = [2, -3.4]^\top\)</span> 和偏差 <span class="math inline">\(b = 4.2\)</span>，以及一个随机噪声项 <span class="math inline">\(\epsilon\)</span> 来生成标签 <span class="math display">\[ \boldsymbol{y} = \boldsymbol{X}\boldsymbol{w} + b + \epsilon \]</span>，其中噪声项 <span class="math inline">\(\epsilon\)</span> 服从均值为0、标准差为0.01的正态分布。噪声代表了数据集中无意义的干扰。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display, embed </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># %matplotlib inline</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Union</span>, <span class="type">List</span>, <span class="type">Tuple</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_train_set</span>(<span class="params">num_inputs: <span class="built_in">int</span>, num_examples: <span class="built_in">int</span>, true_w: <span class="type">List</span>[<span class="built_in">float</span>], true_b: <span class="built_in">float</span></span>) -&gt; <span class="type">List</span>[torch.tensor]:</span></span><br><span class="line">    features = torch.randn(num_examples, num_inputs, dtype = torch.float64)</span><br><span class="line">    labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">    <span class="literal">Ellipsis</span> = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size = labels.size()), dtype = torch.float64)</span><br><span class="line">    labels += <span class="literal">Ellipsis</span></span><br><span class="line">    <span class="keyword">return</span> [features, labels]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    num_inputs = <span class="number">2</span></span><br><span class="line">    num_examples = <span class="number">1000</span></span><br><span class="line">    true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">    true_b = <span class="number">4.2</span></span><br><span class="line">    [features, labels] = generate_train_set(num_inputs, num_examples, true_w, true_b)</span><br><span class="line">    embed()</span><br></pre></td></tr></table></figure>
<p>关于 <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_38355352/article/details/88783832"><code>%matplotlib inline</code></a></p>
<blockquote>
<p>注意 <code>feature</code> 的意义，实际上的 <code>feature</code> 仅仅是特征，不过在这个例子里面，把特征和样本个数糅合在了一起，作为了 <code>features</code></p>
</blockquote>
<p>接下来，设置绘图相关的函数：通过生成第二个特征<code>features[:, 1]</code>和标签 <code>labels</code> 的散点图，可以更直观地观察两者间的线性关系。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; touch d2lzh_pytorch.py</span><br><span class="line">&gt; <span class="built_in">echo</span> <span class="string">&quot;def use_svg_display():</span></span><br><span class="line"><span class="string">    # 用矢量图显示</span></span><br><span class="line"><span class="string">    display.set_matplotlib_formats(&#x27;svg&#x27;)&quot;</span> &gt;&gt; d2lzh_pytorch.py</span><br><span class="line">&gt; <span class="built_in">echo</span> <span class="string">&quot;def set_figsize(figsize=(3.5, 2.5)):</span></span><br><span class="line"><span class="string">    use_svg_display()</span></span><br><span class="line"><span class="string">    # 设置图的尺寸</span></span><br><span class="line"><span class="string">    plt.rcParams[&#x27;figure.figsize&#x27;] = figsize&quot;</span> &gt;&gt; d2lzh_pytorch.py</span><br><span class="line">&gt; cat d2lzh_pytorch.py</span><br><span class="line">def use_svg_display():</span><br><span class="line">    <span class="comment"># 用矢量图显示</span></span><br><span class="line">    display.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line">def set_figsize(figsize=(3.5, 2.5)):</span><br><span class="line">    use_svg_display()</span><br><span class="line">    <span class="comment"># 设置图的尺寸</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = figsize</span><br></pre></td></tr></table></figure>
<p>如上操作后，即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_svg_display</span>():</span></span><br><span class="line">    <span class="comment"># 用矢量图显示</span></span><br><span class="line">    display.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    <span class="comment"># 设置图的尺寸</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = figsize</span><br><span class="line"></span><br><span class="line"><span class="comment"># # 在../d2lzh_pytorch里面添加上面两个函数后就可以这样导入</span></span><br><span class="line"><span class="comment"># import sys</span></span><br><span class="line"><span class="comment"># sys.path.append(&quot;..&quot;)</span></span><br><span class="line"><span class="comment"># from d2lzh_pytorch import * </span></span><br><span class="line"></span><br><span class="line">set_figsize()</span><br><span class="line">plt.scatter(features[:, <span class="number">1</span>].numpy(), labels.numpy(), <span class="number">1</span>);</span><br></pre></td></tr></table></figure>
<p>我破防了，貌似还是<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/33105153">得用 jupyter</a>，因为 display 这些都是服务于 jupyter 的</p>
<hr />
<p>我又破防了，貌似不一定要用 jupyter，我继续用 pycharm 了，不过至少要：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">----------------------</span><br><span class="line">d2lzh_pytorch.py</span><br><span class="line">----------------------</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display, embed</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">use_svg_display</span>():</span></span><br><span class="line">    <span class="comment"># 用矢量图显示</span></span><br><span class="line">    display.set_matplotlib_formats(<span class="string">&#x27;svg&#x27;</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_figsize</span>(<span class="params">figsize=(<span class="params"><span class="number">3.5</span>, <span class="number">2.5</span></span>)</span>):</span></span><br><span class="line">    use_svg_display()</span><br><span class="line">    <span class="comment"># 设置图的尺寸</span></span><br><span class="line">    plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = figsize</span><br></pre></td></tr></table></figure>
<p>这里绘图还是有问题，还得多改改</p>
<hr />
<p>实际上，改的不多，在主文件最后一行加上 <code>plt.show()</code> 即可，效果如下，散的还行，明天继续训练</p>
<p><img src="https://pic.imgdb.cn/item/62090c6e2ab3f51d91cb212d.jpg" style="zoom:25%;" /></p>
<hr />
<p>至于为什么非得写出一个新的文件——</p>
<blockquote>
<p>我们将上面的<code>plt</code>作图函数以及<code>use_svg_display</code>函数和<code>set_figsize</code>函数定义在<code>d2lzh_pytorch</code>包里。以后在作图时，我们将直接调用<code>d2lzh_pytorch.plt</code>。由于<code>plt</code>在<code>d2lzh_pytorch</code>包中是一个全局变量，我们在作图前只需要调用<code>d2lzh_pytorch.set_figsize()</code>即可打印矢量图并设置图的尺寸。</p>
</blockquote>
<h2 id="读取数据">读取数据</h2>
<p>在训练模型的时候，我们需要遍历数据集并不断读取小批量数据样本。这里我们定义一个函数：它每次返回<code>batch_size</code>（批量大小）个随机样本的特征和标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.longTensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br></pre></td></tr></table></figure>
<p>此处使用了 <code>yield</code> 方法，具体的意义参考<a target="_blank" rel="noopener" href="https://www.runoob.com/python3/python3-iterator-generator.html##%E7%94%9F%E6%88%90%E5%99%A8">此链接底部生成器部分</a></p>
<p>让我们测试第一个小批量数据样本并打印。每个批量的特征形状为(10, 2)，分别对应批量大小和输入个数；标签形状为批量大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">    <span class="built_in">print</span>(x, y)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line">----------------------</span><br><span class="line">tensor([[ <span class="number">0.4309</span>, -<span class="number">2.4063</span>],</span><br><span class="line">        [ <span class="number">0.3015</span>,  <span class="number">0.3022</span>],</span><br><span class="line">        [-<span class="number">0.4291</span>, -<span class="number">0.5646</span>],</span><br><span class="line">        [ <span class="number">0.6140</span>, -<span class="number">0.4660</span>],</span><br><span class="line">        [ <span class="number">0.4325</span>, -<span class="number">0.1307</span>],</span><br><span class="line">        [-<span class="number">0.2154</span>,  <span class="number">0.2170</span>],</span><br><span class="line">        [ <span class="number">0.8936</span>,  <span class="number">0.1344</span>],</span><br><span class="line">        [ <span class="number">2.1532</span>,  <span class="number">1.4954</span>],</span><br><span class="line">        [-<span class="number">0.2951</span>, -<span class="number">0.4246</span>],</span><br><span class="line">        [-<span class="number">0.7586</span>, -<span class="number">0.8595</span>]], dtype=torch.float64) tensor([<span class="number">13.2452</span>,  <span class="number">3.7605</span>,  <span class="number">5.2628</span>,  <span class="number">6.9973</span>,  <span class="number">5.5235</span>,  <span class="number">3.0280</span>,  <span class="number">5.5270</span>,  <span class="number">3.4118</span>,</span><br><span class="line">         <span class="number">5.0553</span>,  <span class="number">5.6174</span>], dtype=torch.float64)</span><br></pre></td></tr></table></figure>
<h2 id="初始化模型参数">初始化模型参数</h2>
<p>我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)), dtype=torch.float32)</span><br><span class="line">b = torch.zeros(<span class="number">1</span>, dtype=torch.float32)Copy to clipboardErrorCopied</span><br></pre></td></tr></table></figure>
<p>之后的模型训练中，需要对这些参数求梯度来迭代参数的值，因此我们要让它们的<code>requires_grad=True</code>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">w.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br><span class="line">b.requires_grad_(requires_grad=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure>
<h2 id="定义模型-1">定义模型</h2>
<p>下面是线性回归的矢量计算表达式的实现。我们使用<code>mm</code>函数做矩阵乘法。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linreg</span>(<span class="params">X, w, b</span>):</span>  <span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(X, w) + b</span><br></pre></td></tr></table></figure>
<h2 id="定义损失函数">定义损失函数</h2>
<p>我们使用上一节描述的平方损失来定义线性回归的损失函数。在实现中，我们需要把真实值<code>y</code>变形成预测值<code>y_hat</code>的形状。以下函数返回的结果也将和<code>y_hat</code>的形状相同。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span>(<span class="params">y_hat, y</span>):</span>  <span class="comment"># 本函数已保存在d2lzh_pytorch包中方便以后使用</span></span><br><span class="line">    <span class="comment"># 注意这里返回的是向量, 另外, pytorch里的MSELoss并没有除以 2</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br></pre></td></tr></table></figure>
<h2 id="定义优化算法">定义优化算法</h2>
<p>以下的<code>sgd</code>函数实现了上一节中介绍的小批量随机梯度下降算法。它通过不断迭代模型参数来优化损失函数。这里自动求梯度模块计算得来的梯度是一个批量样本的梯度和。我们将它除以批量大小来得到平均值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">self_gradient_differ</span>(<span class="params">params, lr, batch_size</span>):</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size</span><br></pre></td></tr></table></figure>
<h2 id="训练模型">训练模型</h2>
<p>在训练中，我们将多次迭代模型参数。在每次迭代中，我们根据当前读取的小批量数据样本（特征<code>X</code>和标签<code>y</code>），通过调用反向函数<code>backward</code>计算小批量随机梯度，并调用优化算法<code>sgd</code>迭代模型参数。由于我们之前设批量大小<code>batch_size</code>为10，每个小批量的损失<code>l</code>的形状为(10, 1)。回忆一下自动求梯度一节。由于变量<code>l</code>并不是一个标量，所以我们可以调用<code>.sum()</code>将其求和得到一个标量，再运行<code>l.backward()</code>得到该变量有关模型参数的梯度。注意在每次更新完参数后不要忘了将参数的梯度清零。</p>
<p>在一个迭代周期（epoch）中，我们将完整遍历一遍<code>data_iter</code>函数，并对训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。这里的迭代周期个数<code>num_epochs</code>和学习率<code>lr</code>都是超参数，分别设3和0.03。在实践中，大多超参数都需要通过反复试错来不断调节。虽然迭代周期数设得越大模型可能越有效，但是训练时间可能过长。而有关学习率对模型的影响，我们会在后面“优化算法”一章中详细介绍。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">net = linreg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):  <span class="comment"># 训练模型一共需要num_epochs个迭代周期</span></span><br><span class="line">    <span class="comment"># 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X</span></span><br><span class="line">    <span class="comment"># 和y分别是小批量样本的特征和标签</span></span><br><span class="line">    <span class="keyword">for</span> X, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">        l = loss(net(X, w, b), y).<span class="built_in">sum</span>()  <span class="comment"># l是有关小批量X和y的损失</span></span><br><span class="line">        l.backward()  <span class="comment"># 小批量的损失对模型参数求梯度</span></span><br><span class="line">        sgd([w, b], lr, batch_size)  <span class="comment"># 使用小批量随机梯度下降迭代模型参数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 不要忘了梯度清零</span></span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    train_l = loss(net(features, w, b), labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_l.mean().item()))</span><br><span class="line">    <span class="built_in">print</span>(true_w, <span class="string">&#x27;\n&#x27;</span>, w)</span><br><span class="line">		<span class="built_in">print</span>(true_b, <span class="string">&#x27;\n&#x27;</span>, b)</span><br></pre></td></tr></table></figure>
<p>最后学习成功，我的代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display, embed</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Union</span>, <span class="type">List</span>, <span class="type">Tuple</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">&quot;..&quot;</span>)</span><br><span class="line"><span class="keyword">from</span> d2lzh_pytorch <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_train_set</span>(<span class="params">num_inputs: <span class="built_in">int</span>, num_examples: <span class="built_in">int</span>, true_w: <span class="type">List</span>[<span class="built_in">float</span>], true_b: <span class="built_in">float</span></span>) -&gt; <span class="type">List</span>[torch.tensor]:</span></span><br><span class="line">    features = torch.randn(num_examples, num_inputs, dtype = torch.float64)</span><br><span class="line">    labels = true_w[<span class="number">0</span>] * features[:, <span class="number">0</span>] + true_w[<span class="number">1</span>] * features[:, <span class="number">1</span>] + true_b</span><br><span class="line">    epsilon = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size = labels.size()), dtype = torch.float64)</span><br><span class="line">    labels += epsilon</span><br><span class="line">    <span class="keyword">return</span> [features, labels]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 本函数已保存在d2lzh包中方便以后使用</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter</span>(<span class="params">batch_size, features, labels</span>):</span></span><br><span class="line">    num_examples = <span class="built_in">len</span>(features)</span><br><span class="line">    indices = <span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(indices)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_examples, batch_size):</span><br><span class="line">        j = torch.LongTensor(indices[i: <span class="built_in">min</span>(i + batch_size, num_examples)])</span><br><span class="line">        <span class="keyword">yield</span> features.index_select(<span class="number">0</span>, j), labels.index_select(<span class="number">0</span>, j)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_reg</span>(<span class="params">x, w, b</span>):</span></span><br><span class="line">    <span class="keyword">return</span> torch.mm(x, w) + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">squared_loss</span>(<span class="params">y_hat, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> (y_hat - y.view(y_hat.size())) ** <span class="number">2</span> / <span class="number">2</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">self_gradient_differ</span>(<span class="params">params, lr, batch_size</span>):</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.data -= lr * param.grad / batch_size</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_inputs = <span class="number">2</span></span><br><span class="line">num_examples = <span class="number">1000</span></span><br><span class="line">true_w = [<span class="number">2</span>, -<span class="number">3.4</span>]</span><br><span class="line">true_b = <span class="number">4.2</span></span><br><span class="line">lr = <span class="number">0.03</span></span><br><span class="line">num_epochs = <span class="number">3</span></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">net = linear_reg</span><br><span class="line">loss = squared_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    [features, labels] = generate_train_set(num_inputs, num_examples, true_w, true_b)</span><br><span class="line">    w = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, <span class="number">1</span>)))</span><br><span class="line">    b = torch.zeros(<span class="number">1</span>, dtype = torch.float64)</span><br><span class="line">    w.requires_grad_(requires_grad = <span class="literal">True</span>)</span><br><span class="line">    b.requires_grad_(requires_grad = <span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> data_iter(batch_size, features, labels):</span><br><span class="line">            lossage = loss(net(x, w, b), y).<span class="built_in">sum</span>()</span><br><span class="line">            lossage.backward()</span><br><span class="line">            self_gradient_differ([w, b], lr, batch_size)</span><br><span class="line">            w.grad.data.zero_()</span><br><span class="line">            b.grad.data.zero_()</span><br><span class="line">        train_loss = loss(net(features, w, b), labels)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d, loss %f&#x27;</span> % (epoch + <span class="number">1</span>, train_loss.mean().item()))</span><br><span class="line">    <span class="built_in">print</span>(true_w, <span class="string">&#x27;\n&#x27;</span>, w)</span><br><span class="line">    <span class="built_in">print</span>(true_b, <span class="string">&#x27;\n&#x27;</span>, b)</span><br></pre></td></tr></table></figure>
<h1 id="简洁实现">简洁实现</h1>
<p>稍微踩了一些坑</p>
<h2 id="import-error">import error</h2>
<p>第一天卡了非常久，卡在</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure>
<p>我特么第一天写成了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> optim <span class="keyword">import</span> optim</span><br><span class="line">-----------</span><br><span class="line">ModuleNotFoundError: No module named <span class="string">&#x27;optim&#x27;</span></span><br></pre></td></tr></table></figure>
<p>然后我当时就方了，以为又被 M1 坑了，然后网上找了很久，把 torch 删了又下回来，本地还是跑不了，最后连上 AIR 的服务器，结果因为换源问题，连个 torch 都没 install 成功。</p>
<p>最后，文琦给我一笔指出，我超——tqdm 是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure>
<p>但是，torch 不是啊！！！</p>
<h2 id="float64">float64</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">RuntimeError: expected scalar <span class="built_in">type</span> Double but found Float</span><br></pre></td></tr></table></figure>
<p>中间跑出来了一次 RuntimeError，结果是用了 float64 的缘故，全改成 float32 就好了。</p>
<h2 id="parameter-of-net">parameter of net</h2>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> net.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param)</span><br><span class="line">-------------------------------</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([[-0.0277,  0.2771]], requires_grad=True)</span><br><span class="line">Parameter containing:</span><br><span class="line">tensor([0.3395], requires_grad=True)</span><br></pre></td></tr></table></figure>
<p>我当时觉得很神奇，为什么一层网络居然有两个输出，然而实际上：</p>
<p><img src="https://s2.loli.net/2022/02/23/C3qXKv1aLnt4Bby.png"  /></p>
<blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">  <span class="built_in">print</span> (name, param.data)</span><br><span class="line">--------------------</span><br><span class="line">In [<span class="number">3</span>]: <span class="keyword">for</span> name, param <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">	<span class="built_in">print</span> (name, param.data)</span><br><span class="line">--------------------</span><br><span class="line">linear.weight tensor([[-<span class="number">0.2538</span>,  <span class="number">0.3181</span>]])</span><br><span class="line">linear.bias tensor([-<span class="number">0.0253</span>])</span><br></pre></td></tr></table></figure>
<p>可以输出 name 看看，一层两个 parameter</p>
</blockquote>
<blockquote>
<p><span class="math inline">\(A^T\)</span> 和 <span class="math inline">\(b\)</span> 都是网络参数，<span class="math inline">\(A^T\)</span> 和 input feature 点积是 2 维，然后加 b 得到 output feature，b 是 1 维</p>
</blockquote>
<blockquote>
<p>一层 linear 包含了 weight 跟 bias</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> self.NLP_base.state_dict():</span><br><span class="line">            <span class="built_in">print</span>(x)</span><br></pre></td></tr></table></figure>
<p>可以用这句代码查看每层的名字</p>
</blockquote>
<h1 id="softmax">Softmax</h1>
<p><span class="math display">\[
H\left(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}\right ) = -\sum_{j=1}^q y_j^{(i)} \log \hat y_j^{(i)}
\]</span> 粗体 <span class="math inline">\(\boldsymbol y^{(i)}\)</span> 为向量，也即整个 ground truth 矩阵第 <span class="math inline">\(i\)</span> 行的向量。而带下标的<span class="math inline">\(y_j^{(i)}\)</span>是向量<span class="math inline">\(\boldsymbol y^{(i)}\)</span>中的第 <span class="math inline">\(j\)</span> 个元素，非 0 即 1。</p>
<p>为什么向量<span class="math inline">\(\boldsymbol y^{(i)}\)</span>中只有第<span class="math inline">\(y^{(i)}\)</span>个元素<span class="math inline">\(y^{(i)}_{y^{(i)}}\)</span>为1，其余全为0，于是<span class="math inline">\(H(\boldsymbol y^{(i)}, \boldsymbol {\hat y}^{(i)}) = -\log \hat y*{y^{(i)}}^{(i)}\)</span>。也就是说，交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Eren Zhao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://example.com/2022/01/15/%E7%A3%95%E7%9B%90/Dive%20Into%20Deep%20Learning%20Part%201/">http://example.com/2022/01/15/%E7%A3%95%E7%9B%90/Dive%20Into%20Deep%20Learning%20Part%201/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a><a class="post-meta__tags" href="/tags/2022%E5%AF%92%E5%81%87/">2022寒假</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/DIDL/">DIDL</a></div><div class="post_share"><div class="social-share" data-image="https://pic.imgdb.cn/item/61f0fc6b2ab3f51d91730400.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/01/15/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/%E9%94%BB%E7%82%BC%E6%89%93%E5%8D%A1/"><img class="prev-cover" src="https://pic.imgdb.cn/item/61eccb4d2ab3f51d91d616f7.jpg" onerror="onerror=null;src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">寒假锻炼打卡记录</div></div></a></div><div class="next-post pull-right"><a href="/2022/01/14/%E5%87%BA%E5%9B%BD/reasoning/"><img class="next-cover" src="https://pic.imgdb.cn/item/62270f5c5baa1a80ab3a4417.jpg" onerror="onerror=null;src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">一些人生的规划</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/01/20/%E7%A3%95%E7%9B%90/equivariant/" title="Euivariant & Invariant"><img class="cover" src="https://pic.imgdb.cn/item/61ed142e2ab3f51d911d2e34.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-20</div><div class="title">Euivariant & Invariant</div></div></a></div><div><a href="/2022/01/11/%E7%A3%95%E7%9B%90/paper_reading/" title="如何有效的读一篇论文"><img class="cover" src="https://pic.imgdb.cn/item/61eccc372ab3f51d91d6e87d.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-11</div><div class="title">如何有效的读一篇论文</div></div></a></div><div><a href="/2022/01/09/CS/others/typing/" title="typing"><img class="cover" src="https://pic.imgdb.cn/item/62270f5c5baa1a80ab3a4417.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-09</div><div class="title">typing</div></div></a></div><div><a href="/2022/02/16/CS/others/python_decorator/" title="decorator Python装饰器"><img class="cover" src="https://pic.imgdb.cn/item/61f106922ab3f51d917b2b9f.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-16</div><div class="title">decorator Python装饰器</div></div></a></div><div><a href="/2022/01/10/%E7%A3%95%E7%9B%90/VIT/" title="Vision Transformer Learning Log"><img class="cover" src="https://pic.imgdb.cn/item/61eccc282ab3f51d91d6db18.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-10</div><div class="title">Vision Transformer Learning Log</div></div></a></div><div><a href="/2022/01/12/%E7%A3%95%E7%9B%90/FPN/" title="FPN & RefineNet"><img class="cover" src="https://pic.imgdb.cn/item/61eccc282ab3f51d91d6db18.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-12</div><div class="title">FPN & RefineNet</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend.jpg'" alt="avatar"/></div><div class="author-info__name">Eren Zhao</div><div class="author-info__description">求道之人，不论寒暑，无问西东</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">166</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">53</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">12</div></a></div></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhaochenyang20"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">La vida sola viviras</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E8%A6%81%E6%B1%82"><span class="toc-number">1.</span> <span class="toc-text">学习要求</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#intro"><span class="toc-number">2.</span> <span class="toc-text">intro</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%A8%E6%95%B0%E6%8D%AE%E7%BC%96%E7%A8%8B"><span class="toc-number">2.1.</span> <span class="toc-text">用数据编程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">2.2.</span> <span class="toc-text">深度学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%B5%B7%E6%BA%90"><span class="toc-number">3.</span> <span class="toc-text">起源</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%91%E5%B1%95"><span class="toc-number">4.</span> <span class="toc-text">发展</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%89%B9%E7%82%B9"><span class="toc-number">5.</span> <span class="toc-text">特点</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">6.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">7.</span> <span class="toc-text">环境配置</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%81%BF%E5%BC%80-jupyter"><span class="toc-number">7.1.</span> <span class="toc-text">如何避开 jupyter</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%80%81%E5%AE%9E%E7%94%A8-jupyter"><span class="toc-number">7.2.</span> <span class="toc-text">老实用 jupyter</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C"><span class="toc-number">8.</span> <span class="toc-text">数据操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA-tensor"><span class="toc-number">8.1.</span> <span class="toc-text">创建 Tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%95%B0%E6%93%8D%E4%BD%9C"><span class="toc-number">8.2.</span> <span class="toc-text">算数操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%94%B9%E5%8F%98%E5%BD%A2%E7%8A%B6"><span class="toc-number">8.3.</span> <span class="toc-text">改变形状</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD"><span class="toc-number">8.4.</span> <span class="toc-text">广播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gpu"><span class="toc-number">8.5.</span> <span class="toc-text">GPU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E8%A7%A3%E6%A2%AF%E5%BA%A6"><span class="toc-number">8.6.</span> <span class="toc-text">自动求解梯度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#tensor"><span class="toc-number">8.6.1.</span> <span class="toc-text">Tensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">8.6.2.</span> <span class="toc-text">梯度</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="toc-number">9.</span> <span class="toc-text">深度学习基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">9.1.</span> <span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0"><span class="toc-number">9.1.1.</span> <span class="toc-text">线性回归的基本要素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B"><span class="toc-number">9.1.2.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">9.1.3.</span> <span class="toc-text">模型训练</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">9.1.3.1.</span> <span class="toc-text">(1) 训练数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">9.1.3.2.</span> <span class="toc-text">(2) 损失函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">9.1.3.3.</span> <span class="toc-text">(3) 优化算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%A2%84%E6%B5%8B"><span class="toc-number">9.1.4.</span> <span class="toc-text">模型预测</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="toc-number">9.2.</span> <span class="toc-text">表示方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A2%E9%87%8F%E8%AE%A1%E7%AE%97"><span class="toc-number">9.2.1.</span> <span class="toc-text">矢量计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">9.2.2.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">10.</span> <span class="toc-text">代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">10.1.</span> <span class="toc-text">生成数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE"><span class="toc-number">10.2.</span> <span class="toc-text">读取数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">10.3.</span> <span class="toc-text">初始化模型参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%A8%A1%E5%9E%8B-1"><span class="toc-number">10.4.</span> <span class="toc-text">定义模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">10.5.</span> <span class="toc-text">定义损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">10.6.</span> <span class="toc-text">定义优化算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">10.7.</span> <span class="toc-text">训练模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">11.</span> <span class="toc-text">简洁实现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#import-error"><span class="toc-number">11.1.</span> <span class="toc-text">import error</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#float64"><span class="toc-number">11.2.</span> <span class="toc-text">float64</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#parameter-of-net"><span class="toc-number">11.3.</span> <span class="toc-text">parameter of net</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#softmax"><span class="toc-number">12.</span> <span class="toc-text">Softmax</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/05/02/%E9%9A%8F%E7%AC%94/%E6%B8%85%E5%8D%8E%E5%9B%AD%E6%97%A5%E8%AE%B0/%E6%B8%85%E5%8D%8E%E5%9B%AD%E6%97%A5%E8%AE%B0--%E7%AC%AC%E5%9B%9B%E9%83%A8/" title="清华园日记——第四部"><img src="https://pic.imgdb.cn/item/61eccc372ab3f51d91d6e87d.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="清华园日记——第四部"/></a><div class="content"><a class="title" href="/2022/05/02/%E9%9A%8F%E7%AC%94/%E6%B8%85%E5%8D%8E%E5%9B%AD%E6%97%A5%E8%AE%B0/%E6%B8%85%E5%8D%8E%E5%9B%AD%E6%97%A5%E8%AE%B0--%E7%AC%AC%E5%9B%9B%E9%83%A8/" title="清华园日记——第四部">清华园日记——第四部</a><time datetime="2022-05-01T23:08:23.503Z" title="Created 2022-05-02 07:08:23">2022-05-02</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/25/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/%E6%B1%82%E9%81%93%E4%B9%8B%E4%BA%BA/" title="求道之人，不问寒暑"><img src="https://pic.imgdb.cn/item/623475e95baa1a80ab3b366f.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="求道之人，不问寒暑"/></a><div class="content"><a class="title" href="/2022/04/25/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/%E6%B1%82%E9%81%93%E4%B9%8B%E4%BA%BA/" title="求道之人，不问寒暑">求道之人，不问寒暑</a><time datetime="2022-04-25T00:12:06.667Z" title="Created 2022-04-25 08:12:06">2022-04-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/24/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/aniversary/" title="分流转系后的一整年"><img src="https://pic.imgdb.cn/item/61eccb5c2ab3f51d91d6238c.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="分流转系后的一整年"/></a><div class="content"><a class="title" href="/2022/04/24/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/aniversary/" title="分流转系后的一整年">分流转系后的一整年</a><time datetime="2022-04-24T13:17:30.403Z" title="Created 2022-04-24 21:17:30">2022-04-24</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/15/CS/others/GSOC_proposal/" title="Proposal for Explore and Optimize Popular Datasets for Hub"><img src="https://pic.imgdb.cn/item/61f0fc802ab3f51d91731f03.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="Proposal for Explore and Optimize Popular Datasets for Hub"/></a><div class="content"><a class="title" href="/2022/04/15/CS/others/GSOC_proposal/" title="Proposal for Explore and Optimize Popular Datasets for Hub">Proposal for Explore and Optimize Popular Datasets for Hub</a><time datetime="2022-04-15T02:33:59.508Z" title="Created 2022-04-15 10:33:59">2022-04-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/13/Lecture/2022%20Spring/%E6%88%91%E4%BB%AC%E7%9A%84%E4%BC%9F%E5%A4%A7%E4%B8%8E%E6%B8%BA%E5%B0%8F/" title="我们的伟大与渺小"><img src="https://pic.imgdb.cn/item/61f106b42ab3f51d917b5210.jpg" onerror="this.onerror=null;this.src='https://wkphoto.cdn.bcebos.com/1f178a82b9014a9058a735deb9773912b31bee3c.jpg'" alt="我们的伟大与渺小"/></a><div class="content"><a class="title" href="/2022/04/13/Lecture/2022%20Spring/%E6%88%91%E4%BB%AC%E7%9A%84%E4%BC%9F%E5%A4%A7%E4%B8%8E%E6%B8%BA%E5%B0%8F/" title="我们的伟大与渺小">我们的伟大与渺小</a><time datetime="2022-04-13T10:15:08.391Z" title="Created 2022-04-13 18:15:08">2022-04-13</time></div></div></div></div></div></div></main><footer id="footer" style="background: ＃191970"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Eren Zhao</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Local search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>