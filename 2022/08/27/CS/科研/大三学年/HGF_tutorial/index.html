<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>HuggingFace Transformer Summary | ä½ ä¸€ç”Ÿçš„æ•…äº‹</title><meta name="keywords" content="ç§‘ç ”,æš‘ç ”"><meta name="author" content="Eren Zhao"><meta name="copyright" content="Eren Zhao"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="HuggingFace is a superpowerful library built for various forms of transformers used in NLP tasks.">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Transformer Summary">
<meta property="og:url" content="http://example.com/2022/08/27/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%B8%89%E5%AD%A6%E5%B9%B4/HGF_tutorial/index.html">
<meta property="og:site_name" content="ä½ ä¸€ç”Ÿçš„æ•…äº‹">
<meta property="og:description" content="HuggingFace is a superpowerful library built for various forms of transformers used in NLP tasks.">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Adam Jaime (iciBcD8NOeA).jpg">
<meta property="article:published_time" content="2022-08-27T10:42:13.448Z">
<meta property="article:modified_time" content="2022-08-27T14:15:16.956Z">
<meta property="article:author" content="Eren Zhao">
<meta property="article:tag" content="ç§‘ç ”">
<meta property="article:tag" content="æš‘ç ”">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Adam Jaime (iciBcD8NOeA).jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="http://example.com/2022/08/27/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%B8%89%E5%AD%A6%E5%B9%B4/HGF_tutorial/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"éº»äº†ï¼Œæ‰¾ä¸åˆ°æ‚¨æŸ¥è¯¢çš„å†…å®¹ï¼š${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶é”™è¯¯',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'å¤©',
  date_suffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: {"limitCount":50,"languages":{"author":"ä½œè€…: Eren Zhao","link":"é“¾æ¥: ","source":"æ¥æº: ä½ ä¸€ç”Ÿçš„æ•…äº‹","info":"è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ã€‚å•†ä¸šè½¬è½½è¯·è”ç³»ä½œè€…è·å¾—æˆæƒï¼Œéå•†ä¸šè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: true,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Transformer Summary',
  isPost: true,
  isHome: false,
  isHighlightShrink: true,
  isToc: true,
  postUpdate: '2022-08-27 22:15:16'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><meta name="generator" content="Hexo 6.0.0"><style>.darkmode--activated{--body-bg-color:#282828;--content-bg-color:#333;--card-bg-color:#555;--text-color:#ccc;--blockquote-color:#bbb;--link-color:#ccc;--link-hover-color:#eee;--brand-color:#ddd;--brand-hover-color:#ddd;--table-row-odd-bg-color:#282828;--table-row-hover-bg-color:#363636;--menu-item-bg-color:#555;--btn-default-bg:#222;--btn-default-color:#ccc;--btn-default-border-color:#555;--btn-default-hover-bg:#666;--btn-default-hover-color:#ccc;--btn-default-hover-border-color:#666;--highlight-background:#282b2e;--highlight-foreground:#a9b7c6;--highlight-gutter-background:#34393d;--highlight-gutter-foreground:#9ca9b6}.darkmode--activated img{opacity:.75}.darkmode--activated img:hover{opacity:.9}.darkmode--activated code{color:#69dbdc;background:0 0}button.darkmode-toggle{z-index:9999}.darkmode-ignore,img{display:flex!important}</style></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">åŠ è½½ä¸­...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend.jpg'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">190</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">51</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">30</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/list-100/"><i class="fa-fw fas fa-music"></i><span> TODO</span></a></div><div class="menus_item"><a class="site-page" href="/projects/"><i class="fa-fw fas fa-video"></i><span> Projects</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Adam Jaime (iciBcD8NOeA).jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">ä½ ä¸€ç”Ÿçš„æ•…äº‹</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> æœç´¢</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/list-100/"><i class="fa-fw fas fa-music"></i><span> TODO</span></a></div><div class="menus_item"><a class="site-page" href="/projects/"><i class="fa-fw fas fa-video"></i><span> Projects</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">HuggingFace Transformer Summary</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2022-08-27T10:42:13.448Z" title="å‘è¡¨äº 2022-08-27 18:42:13">2022-08-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2022-08-27T14:15:16.956Z" title="æ›´æ–°äº 2022-08-27 22:15:16">2022-08-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/">ç§‘ç ”</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A7%91%E7%A0%94/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">å­—æ•°æ€»è®¡:</span><span class="word-count">1.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»æ—¶é•¿:</span><span>12åˆ†é’Ÿ</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Transformer Summary"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">é˜…è¯»é‡:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="readme">Readme</h1>
<p>From 2022.08.24, with the help of my mentor in SenseTime, I began to take a quick overview through the famous utils used in NLP nowadays, the Hugging Face Library. And this is my extraction from its <a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter1/1">course website</a>.</p>
<p>For trivial codes, Iâ€™d rather neglect them, so the code that remained in this blog is more vital, at least from my point of view.</p>
<p>Note that most of the contents are extractions from the original course, and my comment along with something vital, would be written in something like:</p>
<div class="note info modern"><p>This is my comment or something vital!</p>
</div>
<div class="note success modern"><p>This is my comment or something vital!</p>
</div>
<div class="note primary modern"><p>This is my comment or something vital!</p>
</div>
<div class="note danger modern"><p>This is my comment or something vital!</p>
</div>
<p>And in concern that some arguments are trivial but worth saying, to complete a whole logical chain, I will use toggling lines in my blog like this:</p>
<div class="hide-toggle" style="border: 1px solid lightblue"><div class="hide-button toggle-title" style="background-color: lightblue;"><i class="fas fa-caret-right fa-fw"></i><span>something trivial </span></div>
    <div class="hide-content"><p>Here would be something trivial but worth saying.</p>
</div></div>
<h1 id="transformer-models">Transformer Models</h1>
<h2 id="common-nlp-tasks"><a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter1/3?fw=pt">Common NLP tasks</a></h2>
<ol type="1">
<li><strong>Classifying whole sentences</strong></li>
<li><strong>Classifying each word in a sentence</strong></li>
<li><strong>Generating text content</strong></li>
<li><strong>Extracting an answer from a text</strong></li>
<li><strong>Developing a new sentence from an input text</strong></li>
</ol>
<p>and so onâ€¦</p>
<h2 id="what-can-transformers-do"><a target="_blank" rel="noopener" href="https://huggingface.co/course/chapter1/3?fw=pt">What can Transformers do?</a></h2>
<p>The most fundamental object in the Transformers library is the <code>pipeline()</code> function. U can efficiently complete your work like this:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> pipeline</span><br><span class="line"></span><br><span class="line">classifier = pipeline(<span class="string">&quot;sentiment-analysis&quot;</span>)</span><br><span class="line">classifier(<span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>)</span><br><span class="line">---------</span><br><span class="line">[&#123;<span class="string">&#x27;label&#x27;</span>: <span class="string">&#x27;POSITIVE&#x27;</span>, <span class="string">&#x27;score&#x27;</span>: <span class="number">0.9598047137260437</span>&#125;]</span><br></pre></td></tr></table></figure>
<div class="note success modern"><p>There are <strong>three main steps</strong> involved when you pass some text to a pipeline:</p>
<ol type="1">
<li>The text is preprocessed into a format the model can understand.</li>
<li>The preprocessed inputs are given to the model.</li>
<li>The model's predictions are post-processed so that you can make sense of them.</li>
</ol>
</div>
<h3 id="pipelines">Pipelines</h3>
<p>Some of the <strong>currently <a target="_blank" rel="noopener" href="https://huggingface.co/transformers/main_classes/pipelines.html">available pipelines</a></strong> are:</p>
<ul>
<li>feature-extraction (get the vector representation of a text)</li>
<li>fill-mask</li>
<li>ner (named entity recognition)</li>
<li>question-answering</li>
<li>sentiment-analysis</li>
<li>summarization</li>
<li>text-generation</li>
<li>translation</li>
<li>zero-shot-classification</li>
</ul>
<div class="note info modern"><p>This basic using is already powerful, but only acquiring these will never provide enough insight for a future Ph.D. student!</p>
</div>
<h2 id="transformer-classes-history-and-concepts">Transformer: classes, history and concepts</h2>
<div data-align="center">
<p><img width="1000" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg"/></p>
</div>
<div data-align="center">
History of Transformers<br/>
</div>
<div data-align="center">
<p><img width="1000" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/model_parameters.png"/></p>
</div>
<div data-align="center">
You can see how they are getting bigger than bigger...<br/>
</div>
<h3 id="categories">Categories</h3>
<div class="note success modern"><p>There are <strong>three categories</strong> of mainstream Transformers:</p>
<ol type="1">
<li>GPT-like (also called <em>auto-regressive</em> Transformer models)</li>
<li>BERT-like (also called <em>auto-encoding</em> Transformer models)</li>
<li>BART/T5-like (also called <em>sequence-to-sequence</em> Transformer models)</li>
</ol>
<p>These are indeed derived from three Transformer Architectures:</p>
<ol type="1">
<li><strong>Encoder-only models</strong>: Good for tasks that require an understanding of the input, such as sentence classification and named entity recognition. <strong>Encoder models</strong> are best suited for tasks requiring knowledge of the whole sentence, such as sentence classification, called entity recognition (and more general word classification), and extractive question answering.</li>
<li><strong>Decoder-only models</strong>: Good for generative tasks such as text generation. <strong>Decoder models</strong> usually revolve around predicting the following word in the sentence. These models are best suited for studies involving text generation.</li>
<li><strong>Encoder-decoder models</strong> or <strong>sequence-to-sequence models</strong>: Good for generative tasks that require input, such as translation or summarization. <strong>Sequence-to-sequence models</strong> are best suited for functions revolving around generating new sentences depending on a given input, such as summarization, translation, or generative question answering.</li>
</ol>
</div>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 35%" />
<col style="width: 51%" />
</colgroup>
<thead>
<tr class="header">
<th>Model</th>
<th>Examples</th>
<th>Tasks</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Encoder</td>
<td>ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa</td>
<td>Sentence classification, named entity recognition, extractive question answering</td>
</tr>
<tr class="even">
<td>Decoder</td>
<td>CTRL, GPT, GPT-2, Transformer XL</td>
<td>Text generation</td>
</tr>
<tr class="odd">
<td>Encoder-decoder</td>
<td>BART, T5, Marian, mBART</td>
<td>Summarization, translation, generative question answering</td>
</tr>
</tbody>
</table>
<h3 id="transfer-learning"><strong>Transfer Learning</strong></h3>
<p>During this process, the model is fine-tuned in a supervised way â€” that is, using human-annotated labels â€” on a given task.</p>
<ul>
<li>The pretrained model was already trained on a dataset that has some similarities with the fine-tuning dataset. The fine-tuning process is thus able to take advantage of the knowledge acquired by the initial model during pretraining (for instance, with NLP problems, the pretrained model will have some kind of statistical understanding of the language you are using for your task).</li>
<li>Since the pretrained model was already trained on lots of data, the fine-tuning requires way fewer data to get decent results.</li>
<li>For the same reason, the amount of time and resources needed to get good results are much lower.</li>
</ul>
<h3 id="the-original-architecture"><strong>The original architecture</strong></h3>
<p>The Transformer architecture was initially designed for translation. During training, the encoder receives inputs (sentences) in a specific language, while the decoder gets the same sentences in the desired target language. In the encoder, the attention layers can use all the words in a sentence (since, as we just saw, the translation of a given word can be dependent on what is after as well as before it in the sentence). The decoder, however, works sequentially and can only pay attention to the words in the sentence that it has already translated (so, only the words before the word currently being generated). For example, when we have predicted the first three words of the translated target, we give them to the decoder, which then uses all the inputs of the encoder to try to predict the fourth word.</p>
<h3 id="architectures-vs.-checkpoints"><strong>Architectures vs. checkpoints</strong></h3>
<ul>
<li><strong>Architecture</strong>: This is the skeleton of the model â€” the definition of each layer and each operation that happens within the model.</li>
<li><strong>Checkpoints</strong>: These are the weights that will be loaded in a given architecture.</li>
<li><strong>Model</strong>: This is an umbrella term that isnâ€™t as precise as â€œarchitectureâ€ or â€œcheckpointâ€: it can mean both. This course will specify <em>architecture</em> or <em>checkpoints</em> when it matters to reduce ambiguity.</li>
</ul>
<h3 id="bias-and-limitations"><strong>Bias and Limitations</strong></h3>
<p>When you use these tools, you, therefore, need to keep in the back of your mind that the original model you are using could very quickly generate sexist, racist, or homophobic content. Fine-tuning the model on your data wonâ€™t make this intrinsic bias disappear.</p>
<h1 id="using-transformers">Using Transformers</h1>
<p>Through this part, at least U should know these things:</p>
<div class="note success modern"><ol type="1">
<li>Learned the basic building blocks of a Transformer model.</li>
<li>Learned what makes up a tokenization pipeline.</li>
<li>Saw how to use a Transformer model in practice.</li>
<li>Learned how to leverage a tokenizer to convert text to tensors that are understandable by the model.</li>
<li>Set up a tokenizer and a model together to get from text to predictions.</li>
<li>Learned the limitations of input IDs, and learned about attention masks.</li>
<li>Played around with versatile and configurable tokenizer methods.</li>
</ol>
</div>
<p>The Transformers library was created with the goal of providing a single API through which any Transformer model can be loaded, trained, and saved.</p>
<h2 id="behind-the-pipeline">Behind the pipeline</h2>
<h3 id="preprocessing-with-a-tokenizer">Preprocessing with a tokenizer</h3>
<p>The tokenizer is responsible for:</p>
<ul>
<li>Splitting the input into words, subwords, or symbols (like punctuation) that are called <em>tokens</em></li>
<li>Mapping each token to an integer</li>
<li>Adding additional inputs that may be useful to the model</li>
</ul>
<h3 id="model-heads-making-sense-out-of-numbers">Model heads: Making sense out of numbers</h3>
<p>The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers.</p>
<div data-align="center">
<p><img width="1000" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg"/></p>
</div>
<div data-align="center">
The output of the Transformer model is sent directly to the model head to be processed.<br/>
</div>
<h3 id="postprocessing-the-output">Postprocessing the output</h3>
<p>The values we get as output from our model donâ€™t necessarily make sense by themselves. For instance, we usually get logits output from the model, which need to be fed to a softmax layer, thus ending in probabilities that make sense to human beings.</p>
<h2 id="tokenizers">Tokenizers</h2>
<p>Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data.</p>
<p>A typical tokenization algorithm I have touched on is Subword tokenization, which relies on the principle that frequently used words should not be split into smaller subwords. Still, rare words should be decomposed into meaningful subwords.</p>
<div class="note info modern"><p>I thought for a while.</p>
<p>Okay, the main reason I love it is that I have taken the Scientific English course at Tsinghua University, where I study a lot of roots and affixes.</p>
</div>
<h3 id="encoding">Encoding</h3>
<p>Translating text to numbers is known as <em>encoding</em>. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.</p>
<h3 id="a-basic-pipeline">A Basic Pipeline</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">checkpoint = <span class="string">&quot;distilbert-base-uncased-finetuned-sst-2-english&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(checkpoint)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(checkpoint)</span><br><span class="line">sequences = [<span class="string">&quot;I&#x27;ve been waiting for a HuggingFace course my whole life.&quot;</span>, <span class="string">&quot;So have I!&quot;</span>]</span><br><span class="line"></span><br><span class="line">tokens = tokenizer(sequences, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">output = model(**tokens)</span><br></pre></td></tr></table></figure>
<h1 id="fine-tuning-a-pretrained-model">Fine-Tuning a pretrained Model</h1>
<p>This chapterâ€™s primary purpose is to fine-tune a pretrained model for your own dataset.</p>
<p>At least, U should attain these things:</p>
<div class="note info modern"><ul>
<li>Learned about datasets in the <a target="_blank" rel="noopener" href="https://huggingface.co/datasets">Hub</a> while Learning how to load and preprocess datasets, including using dynamic padding and collators</li>
<li>How to use the high-level <code>Trainer</code> API to fine-tune a model, and implement a lower-level training loop</li>
<li>How to leverage the ğŸ¤— Accelerate library to run that custom training loop on any distributed setup efficiently, which quickly adapts your training loop, so it works for multiple GPUs or TPUs</li>
<li>Implemented your own fine-tuning and evaluation of a model</li>
</ul>
</div>
<h2 id="processing-the-data">Processing the Data</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">raw_datasets = load_dataset(<span class="string">&quot;glue&quot;</span>, <span class="string">&quot;mrpc&quot;</span>)</span><br><span class="line">raw_datasets</span><br><span class="line"></span><br><span class="line">---------</span><br><span class="line"></span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">3668</span></span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">408</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">1725</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<div class="note info modern"><p>We also call features columns. Just like in database systems, we have a similar concept: field.</p>
</div>
<p>We can access some examples as follow:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_train_dataset = raw_datasets[<span class="string">&quot;train&quot;</span>]</span><br><span class="line">raw_train_dataset[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">---------------</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;idx&#x27;</span>: <span class="number">0</span>,</span><br><span class="line"> <span class="string">&#x27;label&#x27;</span>: <span class="number">1</span>,</span><br><span class="line"> <span class="string">&#x27;sentence1&#x27;</span>: <span class="string">&#x27;Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;sentence2&#x27;</span>: <span class="string">&#x27;Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_train_dataset.features</span><br><span class="line"></span><br><span class="line">---------------</span><br><span class="line"></span><br><span class="line">&#123;<span class="string">&#x27;sentence1&#x27;</span>: Value(dtype=<span class="string">&#x27;string&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>),</span><br><span class="line"> <span class="string">&#x27;sentence2&#x27;</span>: Value(dtype=<span class="string">&#x27;string&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>),</span><br><span class="line"> <span class="string">&#x27;label&#x27;</span>: ClassLabel(num_classes=<span class="number">2</span>, names=[<span class="string">&#x27;not_equivalent&#x27;</span>, <span class="string">&#x27;equivalent&#x27;</span>], names_file=<span class="literal">None</span>, <span class="built_in">id</span>=<span class="literal">None</span>),</span><br><span class="line"> <span class="string">&#x27;idx&#x27;</span>: Value(dtype=<span class="string">&#x27;int32&#x27;</span>, <span class="built_in">id</span>=<span class="literal">None</span>)&#125;</span><br></pre></td></tr></table></figure>
<p>Here comes the most important API in this part:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenize_function</span>(<span class="params">example</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tokenizer(example[<span class="string">&quot;sentence1&quot;</span>], example[<span class="string">&quot;sentence2&quot;</span>], truncation=<span class="literal">True</span>)</span><br><span class="line">tokenized_datasets = raw_datasets.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line">tokenized_datasets</span><br><span class="line"></span><br><span class="line">--------</span><br><span class="line"></span><br><span class="line">DatasetDict(&#123;</span><br><span class="line">    train: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>, <span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;token_type_ids&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">3668</span></span><br><span class="line">    &#125;)</span><br><span class="line">    validation: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>, <span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;token_type_ids&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">408</span></span><br><span class="line">    &#125;)</span><br><span class="line">    test: Dataset(&#123;</span><br><span class="line">        features: [<span class="string">&#x27;attention_mask&#x27;</span>, <span class="string">&#x27;idx&#x27;</span>, <span class="string">&#x27;input_ids&#x27;</span>, <span class="string">&#x27;label&#x27;</span>, <span class="string">&#x27;sentence1&#x27;</span>, <span class="string">&#x27;sentence2&#x27;</span>, <span class="string">&#x27;token_type_ids&#x27;</span>],</span><br><span class="line">        num_rows: <span class="number">1725</span></span><br><span class="line">    &#125;)</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure>
<div class="note success modern"><p>From my point, <code>DataSet.map()</code> is used to fix or add a column for your current dataset.</p>
<p>If the mapping function return columns that your dataset hasnâ€™t included, then it would create new columns, like the former example, which adds attention_mask and idx conducting a new dataset.</p>
<p>Otherwise, if the mapping function return columns that your dataset has already included, then it would rewrite the old columns, which we would see it later.</p>
<p>And furthermore, later, we will also see the mapping function usually return a dictionary, like this:</p>
</div>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">ç§‘ç ”</a><a class="post-meta__tags" href="/tags/%E6%9A%91%E7%A0%94/">æš‘ç ”</a></div><div class="post_share"><div class="addthis_inline_share_toolbox"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-62b11572b25ab3ab" async="async"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/31/%E9%9A%8F%E7%AC%94/%E6%B8%85%E5%8D%8E%E5%9B%AD%E6%97%A5%E8%AE%B0/dairy_THU_8/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Johannes Plenio (2TQwrtZnl08).jpg" onerror="onerror=null;src='https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Oscar Rodrigo Hernandez Panczenko (P0GQ95huhBo).jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">ä¸Šä¸€ç¯‡</div><div class="prev_info"></div></div></a></div><div class="next-post pull-right"><a href="/2022/08/20/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%B8%89%E5%AD%A6%E5%B9%B4/chain_of_thought/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/pic_embed/22_08_22_4.jpg" onerror="onerror=null;src='https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Oscar Rodrigo Hernandez Panczenko (P0GQ95huhBo).jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">ä¸‹ä¸€ç¯‡</div><div class="next_info">Chain of Thought Working Notes</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><div><a href="/2022/08/20/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%B8%89%E5%AD%A6%E5%B9%B4/chain_of_thought/" title="Chain of Thought Working Notes"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/pic_embed/22_08_22_4.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-20</div><div class="title">Chain of Thought Working Notes</div></div></a></div><div><a href="/2022/01/21/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%B8%89%E5%AD%A6%E5%B9%B4/Summer%20research/" title="Summer Research 2023"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by v2osk (JE01L3hB0GQ).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-21</div><div class="title">Summer Research 2023</div></div></a></div><div><a href="/2022/08/16/CS/%E7%A7%91%E7%A0%94/%E5%A4%A7%E4%B8%89%E5%AD%A6%E5%B9%B4/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4%E7%A7%91%E7%A0%94%E6%80%BB%E7%BB%93/" title="å¤§äºŒå­¦å¹´çš„ç§‘ç ”å¿ƒè·¯"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Byron Johnson (ec2SZSGPwJA).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-16</div><div class="title">å¤§äºŒå­¦å¹´çš„ç§‘ç ”å¿ƒè·¯</div></div></a></div><div><a href="/2022/07/23/%E9%9A%8F%E7%AC%94/%E5%BF%83%E5%BF%83%E5%BF%B5%E5%BF%B5/talk_with_dg/" title="äº¤æµçè®°â€”â€”é‚“ä¿Šè¾‰è€å¸ˆï¼ˆäºŒï¼‰"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Patrick Schneider (5eYdj1ZTOlc).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-07-23</div><div class="title">äº¤æµçè®°â€”â€”é‚“ä¿Šè¾‰è€å¸ˆï¼ˆäºŒï¼‰</div></div></a></div><div><a href="/2022/06/22/Lecture/2022%20Spring/%E5%9B%BE%E5%BD%A2%E5%AD%A6%E5%A4%A7%E4%BD%9C%E4%B8%9A%E6%8A%A5%E5%91%8A/" title="å›¾å½¢å­¦å¼€æºæŠ¥å‘Š"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Jakob Owens (FKyHyNowp-4).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-22</div><div class="title">å›¾å½¢å­¦å¼€æºæŠ¥å‘Š</div></div></a></div><div><a href="/2022/05/24/Lecture/2022%20Spring/Reinforcement/" title="é‡åŠ›å››å­æ£‹"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Aram Grigoryan (2f_vdmnjLyc).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-05-24</div><div class="title">é‡åŠ›å››å­æ£‹</div></div></a></div><div><a href="/2022/03/15/Lecture/2022%20Spring/Introduction_to_AI/" title="Introduction to Artificial Intelligence"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.imgdb.cn/item/62270f075baa1a80ab393529.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-15</div><div class="title">Introduction to Artificial Intelligence</div></div></a></div><div><a href="/2022/03/23/Lecture/2022%20Spring/Input%20method/" title="How Do We Train An Input Method"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Pascal Debrunner (ELv8fvulR0g).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-03-23</div><div class="title">How Do We Train An Input Method</div></div></a></div><div><a href="/2022/06/06/Lecture/2022%20Spring/deep_learning/" title="Segement Me If U Can"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://raw.githubusercontent.com/zhaochenyang20/ivue_wallpaper/main/ivue_desktop/Photo by Michele Mescolin (lxbeHfdLQB8).jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-06-06</div><div class="title">Segement Me If U Can</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> è¯„è®º</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#readme"><span class="toc-number">1.</span> <span class="toc-text">Readme</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer-models"><span class="toc-number">2.</span> <span class="toc-text">Transformer Models</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#common-nlp-tasks"><span class="toc-number">2.1.</span> <span class="toc-text">Common NLP tasks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#what-can-transformers-do"><span class="toc-number">2.2.</span> <span class="toc-text">What can Transformers do?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#pipelines"><span class="toc-number">2.2.1.</span> <span class="toc-text">Pipelines</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer-classes-history-and-concepts"><span class="toc-number">2.3.</span> <span class="toc-text">Transformer: classes, history and concepts</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#categories"><span class="toc-number">2.3.1.</span> <span class="toc-text">Categories</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transfer-learning"><span class="toc-number">2.3.2.</span> <span class="toc-text">Transfer Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#the-original-architecture"><span class="toc-number">2.3.3.</span> <span class="toc-text">The original architecture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#architectures-vs.-checkpoints"><span class="toc-number">2.3.4.</span> <span class="toc-text">Architectures vs. checkpoints</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#bias-and-limitations"><span class="toc-number">2.3.5.</span> <span class="toc-text">Bias and Limitations</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#using-transformers"><span class="toc-number">3.</span> <span class="toc-text">Using Transformers</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#behind-the-pipeline"><span class="toc-number">3.1.</span> <span class="toc-text">Behind the pipeline</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#preprocessing-with-a-tokenizer"><span class="toc-number">3.1.1.</span> <span class="toc-text">Preprocessing with a tokenizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#model-heads-making-sense-out-of-numbers"><span class="toc-number">3.1.2.</span> <span class="toc-text">Model heads: Making sense out of numbers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#postprocessing-the-output"><span class="toc-number">3.1.3.</span> <span class="toc-text">Postprocessing the output</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#tokenizers"><span class="toc-number">3.2.</span> <span class="toc-text">Tokenizers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#encoding"><span class="toc-number">3.2.1.</span> <span class="toc-text">Encoding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#a-basic-pipeline"><span class="toc-number">3.2.2.</span> <span class="toc-text">A Basic Pipeline</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#fine-tuning-a-pretrained-model"><span class="toc-number">4.</span> <span class="toc-text">Fine-Tuning a pretrained Model</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#processing-the-data"><span class="toc-number">4.1.</span> <span class="toc-text">Processing the Data</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: ï¼ƒ0096FF"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2022 By Eren Zhao</div><div class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><script type="text/javascript" id="maid-script" src="https://unpkg.com/mermaid@undefined/dist/mermaid.min.js?v=undefined"></script><script>if (window.mermaid) {
  var options = JSON.parse(document.getElementById('maid-script').getAttribute('mermaidoptioins'));
  mermaid.initialize(options);
}</script></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æµ…è‰²å’Œæ·±è‰²æ¨¡å¼è½¬æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="chat_btn" type="button" title="rightside.chat_btn"><i class="fas fa-sms"></i></button><a id="to_comment" href="#post-comment" title="ç›´è¾¾è¯„è®º"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">æœ¬åœ°æœç´¢</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  æ•°æ®åº“æ­£åœ¨è‰°éš¾è¿è¡Œ</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="æœç´¢æ–‡ç« " type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '978121c7b834efdd76be',
      clientSecret: '59b40e8f39a1c33db5a2c891771086164b9575c4',
      repo: 'zhaochenyang20.github.io',
      owner: 'zhaochenyang20',
      admin: ['zhaochenyang20'],
      id: '7d2ffaaf93ccdd91253806e56872e522',
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !true) {
  if (true) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><canvas class="fireworks" mobile="true"></canvas><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/fireworks.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/"});</script></body></html>